<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SonicSense: Soundscape Fidelity Tracker</title>
    <meta name="description" content="Track your sonic environment with Sound Diet Tracker">
    <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@400;700&amp;family=Roboto:wght@300;400;600&amp;display=swap"
      rel="stylesheet">
    <style>
    :root {
      --mechanical: #3a506b;
      --nature: #5bc0be;
      --accent: #e8c547;
      --dark: #1c2541;
      --light: #f8f9fa;
      --panel-bg: rgba(30, 35, 40, 0.85);
      --panel-border: 1px solid rgba(90, 140, 150, 0.3);
    }
    
    body {
      font-family: 'Roboto', sans-serif;
      background: linear-gradient(135deg, var(--dark) 0%, var(--mechanical) 100%);
      color: var(--light);
      min-height: 100vh;
      margin: 0;
      padding: 20px;
      background-image: 
        radial-gradient(circle at 10% 20%, rgba(91, 192, 190, 0.1) 0%, transparent 20%),
        linear-gradient(to bottom, transparent 95%, rgba(91, 192, 190, 0.15) 100%);
    }

    .app-container {
      max-width: 1200px;
      margin: 0 auto;
      display: grid;
      grid-template-columns: 1fr;
      gap: 20px;
    }

    header {
      text-align: center;
      margin-bottom: 20px;
    }

    .logo {
      font-family: 'Orbitron', sans-serif;
      font-size: 2.2rem;
      font-weight: 700;
      color: var(--accent);
      text-shadow: 0 0 10px rgba(91, 192, 190, 0.5);
      letter-spacing: 2px;
      margin-bottom: 10px;
    }

    .logo span {
      color: var(--nature);
    }
      
.logo small {
  font-size: 1.2rem;
  font-weight: 400;
  letter-spacing: 0;
  color: var(--nature);
}
      
    .panel {
      background: var(--panel-bg);
      border: var(--panel-border);
      border-radius: 8px;
      padding: 20px;
      box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);
      backdrop-filter: blur(8px);
      position: relative;
      overflow: hidden;
    }

    .panel::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      height: 3px;
      background: linear-gradient(90deg, var(--nature), var(--accent));
    }

    .panel-title {
      font-family: 'Orbitron', sans-serif;
      color: var(--nature);
      margin-top: 0;
      margin-bottom: 15px;
      font-size: 1.2rem;
      display: flex;
      align-items: center;
      gap: 8px;
    }

    .panel-title svg {
      width: 20px;
      height: 20px;
      fill: currentColor;
    }

    .pie-container {
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      padding: 15px;
    }

    .compact-upload {
      padding: 15px;
      text-align: center;
    }

    .drop-zone {
      border: 2px dashed var(--nature);
      padding: 15px;
      text-align: center;
      margin-bottom: 10px;
      border-radius: 8px;
      background: rgba(91, 192, 190, 0.05);
      transition: all 0.3s ease;
      cursor: pointer;
    }

    .drop-zone.active {
      background: rgba(91, 192, 190, 0.15);
      border-color: var(--accent);
    }

    .drop-zone p {
      margin: 5px 0;
      font-size: 0.9rem;
    }

    button {
      background: linear-gradient(to bottom, var(--nature), #4a9e9c);
      color: var(--dark);
      border: none;
      padding: 10px 15px;
      border-radius: 6px;
      font-family: 'Orbitron', sans-serif;
      font-weight: 600;
      font-size: 0.9rem;
      cursor: pointer;
      transition: all 0.3s ease;
      box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
      display: flex;
      align-items: center;
      gap: 8px;
      justify-content: center;
    }

    button:hover {
      transform: translateY(-2px);
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
    }

    button:disabled {
      opacity: 0.6;
      cursor: not-allowed;
    }

    button.secondary {
      background: linear-gradient(to bottom, #4B5EAA, #3a4c8a);
      color: white;
    }

    .button-group {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 10px;
      margin-top: 10px;
    }

    .waveform {
      height: 60px;
      background: rgba(28, 37, 65, 0.5);
      margin: 10px 0;
      border-radius: 4px;
      border: 1px solid rgba(91, 192, 190, 0.2);
    }

    .slider-container {
      margin: 10px 0;
    }

    .slider-label {
      display: flex;
      justify-content: space-between;
      margin-bottom: 5px;
      font-size: 0.9rem;
    }

    input[type="range"] {
      width: 100%;
      height: 6px;
      background: rgba(91, 192, 190, 0.2);
      border-radius: 3px;
      -webkit-appearance: none;
    }

    input[type="range"]::-webkit-slider-thumb {
      -webkit-appearance: none;
      width: 14px;
      height: 14px;
      background: var(--nature);
      border-radius: 50%;
      cursor: pointer;
    }

    .status-indicator {
      padding: 8px;
      border-radius: 4px;
      background: rgba(28, 37, 65, 0.7);
      border-left: 4px solid var(--nature);
      margin: 10px 0;
      font-size: 0.9rem;
      text-align: center;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 10px;
      font-size: 0.9rem;
    }

    th, td {
      padding: 10px 12px;
      text-align: left;
      border-bottom: 1px solid rgba(91, 192, 190, 0.2);
    }

    th {
      background: rgba(28, 37, 65, 0.7);
      font-family: 'Orbitron', sans-serif;
      font-weight: 400;
      color: var(--accent);
      font-size: 0.85rem;
    }

    tr:hover {
      background: rgba(91, 192, 190, 0.05);
    }

    .natural { background: rgba(76, 175, 80, 0.2); }
    .manmade { background: rgba(244, 67, 54, 0.2); }

    input[type="file"] {
      width: 0.1px;
      height: 0.1px;
      opacity: 0;
      overflow: hidden;
      position: absolute;
      z-index: -1;
    }

    .file-input-label {
      display: inline-block;
      padding: 8px 12px;
      background: rgba(91, 192, 190, 0.1);
      border-radius: 4px;
      cursor: pointer;
      font-size: 0.9rem;
      margin-top: 5px;
    }

    .file-input-label:hover {
      background: rgba(91, 192, 190, 0.2);
    }

    /* New Control Panel Styles */
    .control-panel {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 15px;
      margin-bottom: 20px;
    }

    .control-section {
      display: flex;
      flex-direction: column;
      gap: 10px;
    }

    .control-section h3 {
      margin: 0 0 10px 0;
      color: var(--accent);
      font-family: 'Orbitron', sans-serif;
      font-size: 1rem;
    }
      
      .training-controls {
  display: flex;
  flex-direction: column;
  gap: 12px;
}

.training-progress {
  display: flex;
  align-items: center;
  gap: 10px;
  margin: 10px 0;
}

.training-progress progress {
  flex-grow: 1;
  height: 8px;
  border-radius: 4px;
  overflow: hidden;
}

.training-progress progress::-webkit-progress-bar {
  background-color: rgba(91, 192, 190, 0.2);
}

.training-progress progress::-webkit-progress-value {
  background: linear-gradient(to right, var(--nature), var(--accent));
}

.training-results {
  padding: 10px;
  background: rgba(28, 37, 65, 0.7);
  border-radius: 4px;
  margin-bottom: 10px;
  min-height: 20px;
}

.input-group {
  display: flex;
  gap: 8px;
}

.input-group input {
  flex-grow: 1;
  padding: 8px 12px;
  background: var(--panel-bg);
  border: 1px solid var(--nature);
  color: white;
  border-radius: 4px;
}

    @media (max-width: 768px) {
      .control-panel {
        grid-template-columns: 1fr;
      }
    }
  </style>
  </head>
  <body>
    <div class="app-container">
      <header>
        <div class="logo">SonicSense<br>
          <small>soundscape fidelity tracker</small></div>
      </header>
      <!-- Control Panel at the Top -->
      <!-- Help Section --> <button id="openHelpBtn" class="secondary">Open
        Help</button>
      <div class="panel" id="helpSection" style="display: none;">
        <h2 class="panel-title">üìñ How to use SonicSense</h2>
        <h3>In a nutshell</h3>
        <p>SonicSense uses several audio input methods to analyse and classify sounds  
        into <strong>Natural</strong> (birds, rain, wind) or <strong>Human-made</strong>
          (traffic, machines). Using an AI model, it tags the analysis sound results with
          corresponding labels (e.g train, speak, horns, birds etc) while optionally renders, using various playback modes,
          soundscape compositions based on the analysed audio files. Moreover, it connects the analysis tags with external MIDI devices. This is still an experimental feature, but already sounds
          promising for future creative implementations. </p>
        <h3>Core Idea </h3>
        <p> In an era defined by ecological decline and acoustic saturation, how
          we listen becomes as critical as what we measure. SonicSence is an
          AI-powered web application designed to classify environmental audio
          into ‚Äúnatural‚Äù and ‚Äúman-made‚Äù categories in real time or using
          soundscape recordings. Built on MediaPipe‚Äôs YAMNet model, the system
          enables users to record, analyze and visualize their surrounding
          soundscapes, as intuitive pie charts and composable audio mosaics. The AI model can be further trained by the user to include new sound tags.</p>
        <h3>Applications</h3>
        <ol>
          <li><strong>Nature Conservation &amp; Bioacoustics:</strong> Study
            wildlife and ecosystem health by analyzing natural soundscapes.</li>
          <li><strong>Music &amp; Sound Design:</strong> Sample and categorize
            real-world sounds for music production.</li>
          <li><strong>Smart Home &amp; IoT Automation:</strong> Trigger smart
            devices based on sound classifications (Using MIDI Out).</li>
          <li><strong>Field Recording for Film &amp; Games:</strong> Build a
            library of categorized sound effects.</li>
          <li><strong>Scientific Research:</strong> Study human-made vs. natural
            sound ratios in different habitats.</li>
        </ol>
        <h3>Quick start</h3>
        <ol>
          <li><strong>Load the model</strong> ‚Äì wait for the green "Model
            loaded" message.</li>
          <li><strong>Add audio</strong>
            <ul>
              <li><strong>Drag &amp; drop</strong> files or folders onto the
                dashed zone <strong>(currently up to 10Mb size each)</strong>.</li>
              <li><strong>Record live</strong> ‚Äì click <em>üé§ Record Live Audio</em>
                and talk or play sounds around you.</li>
              <li><strong>Analyse one file</strong> ‚Äì pick a single WAV/MP3 and
                press <em>üîç Analyze Single Recording</em>.</li>
              <li><strong>Realtime Analysis</strong> ‚Äì Listen to live audio for
                up to 24 hours, then <em>üîç analyse the soundscape</em>.</li>
            </ul>
          </li>
          <li><strong>Check the tables &amp; chart</strong> ‚Äì rows show the top
            labels, category, and confidence over time.</li>
        </ol>
        <h3>Buttons &amp; controls explained</h3>
        <table>
          <thead>
            <tr>
              <th>Button / Control</th>
              <th>What it does</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Record Live Audio üé§ </td>
              <td>Starts / stops the microphone and streams classification
                results in real time. File split into 5MB chunks and processed
                sequentially. </td>
            </tr>
            <tr>
              <td>Start Real-Time Analysis üéô </td>
              <td>Records continuously for the minutes you specify, then slices
                the audio into 0.975-second segments for analysis.</td>
            </tr>
            <tr>
              <td>Stop ‚èπ </td>
              <td>Stops the current real-time recording and immediately
                processes the segments.</td>
            </tr>
            <tr>
              <td>Play üîä </td>
              <td>Appears next to every real-time or single-file segment.
                Instantly plays that 0.975-second slice through your speakers.</td>
            </tr>
            <tr>
              <td>Download ‚¨á </td>
              <td>Downloads the individual segment as a WAV file (or, for
                real-time, the full recording when you click the large <em>Download
                  Full Recording</em> button).</td>
            </tr>
            <tr>
              <td>Render Combined Audio üéµ </td>
              <td>After you have analysed several files, creates a single long
                WAV that interleaves human-made and natural sounds sorted by
                pitch, with smooth fades between clips. </td>
              <td><strong>Rendering Order:</strong> <br> <br>
                During the rendering process, the audio segments are ordered according to the sequence generated by several algorithms like. <br><br>
               For example, <strong>The Lorenz mode </strong> uses the Lorenz attractor equations to generate a sequence of indices. This sequence is then used to reorder the audio segments. 
                The Lorenz attractor is known for its chaotic behavior, which means the sequence will be highly unpredictable and dynamic. <br> <br>
              The <strong>Markov mode </strong> uses a transition matrix to generate a sequence of audio segments. 
                It starts with a random segment and transitions to the next segment based on the probabilities defined in the transition matrix.
              </td>
            </tr>
            <tr>
              <td>Play ‚ñ∂  / Pause ‚è∏ </td>
              <td>Controls playback of the rendered combined track.</td>
            </tr>
            <tr>
              <td>Fade Duration slider</td>
              <td>Length of the cross-fade between clips in the combined mix
                (0‚Äì5 s).</td>
            </tr>
            <tr>
              <td>Playback Speed slider</td>
              <td>Speed multiplier for the combined track (0.5√ó ‚Üí 2√ó).</td>
            </tr>
            <tr>
              <td>Render time</td>
              <td>Select the Render time of the combined file, up to 10 minutes.
                If sample time is less, samples will loop to fill the required
                time.</td>
            </tr>
            <tr>
              <td>Playback Modes</td>
              <td>Select the order of files in the Render Combined. By
                classification, Random, using a Lorenz Attractor model and more.
              </td>
            </tr>
            <tr>
              <td>MIDI Mapper</td>
              <td>Connect classification tags with MIDI Out. Select MIDI Port,
                Press ADD Mapping, Enter the tag label, Select MIDI Note or
                Control Change Number. Still on development. </td>
            </tr>
          </tbody>
        </table>
        <h3>Tips &amp; tricks</h3>
        <ul>
          <li>Short, clean recordings (&lt; 10 MB) give the best classification.</li>
          <li>Use <strong>headphones</strong> while doing real-time analysis to
            avoid feedback.</li>
          <li>Challenge yourself: try to reach 5 high-confidence "Natural"
            segments in one day!</li>
          <li>Export your daily mix and listen back ‚Äì it's a sonic diary of your
            environment.</li>
        </ul>
        <h3>Training &amp; Using Custom Models (v2 ‚Äì enhanced)</h3>
        <ol>
          <li><strong>Collect samples</strong> ‚Äì gather 5-20 short WAV/MP3 clips
            <strong>(16 kHz, mono, ‚â• 1 s, ‚â§ 10 MB)</strong> that clearly represent the new sound
            (e.g. ‚ÄúCoffee Machine‚Äù, ‚ÄúDog Bark‚Äù). <em>Tip:</em> the more varied
            the clips (different volumes, slight background noise, etc.), the
            better.</li>
          <li><strong>Upload &amp; Train</strong> ‚Äì in the <em>Train Custom
              Sounds</em> panel:
            <ul>
              <li>Click <em>Select Training Samples</em> and choose the clips.</li>
              <li>Enter a short, unique category name.</li>
              <li>Press <em>Train</em>. The app augments your data
                (time-stretch &amp; noise) and shows an accuracy score. Aim for
                ‚â• 70 %; if lower, add more varied samples.</li>
            </ul>
          </li>
          <li><strong>Save / Load</strong>
            <ul>
              <li>When satisfied, press <em>Save Model</em> to download a JSON
                file.</li>
              <li>To reuse later, press <em>Load Model</em> and pick the JSON
                file. The custom classifier is restored instantly.</li>
            </ul>
          </li>
          <li><strong>Use immediately</strong> ‚Äì every new recording or file is
            analysed by **both** the built-in YAMNet and your custom model; the
            best-scoring label is displayed.</li>
          <li><strong>Iterate</strong> ‚Äì add more clips or new categories at any
            time; press <em>Save Model</em> again to keep the updated model.</li>
        </ol>
        <button id="closeHelpBtn" class="secondary">Close Help</button> </div>
      <!-- Audio Studio (vertically centered) -->
      <div class="panel" style="display: flex; flex-direction: column; justify-content: center;">
        <h2 class="panel-title">üéõÔ∏è Audio Studio</h2>
        <div class="control-panel" style="display: flex; flex-direction: column; align-items: center; gap: 15px;">
          <!-- Audio Input -->
          <div class="control-section" style="display: flex; flex-direction: column; align-items: center; width: 100%; max-width: 400px;">
            <h3>Sample Folder Input</h3>
            <div id="dropZone" class="drop-zone" style="width: 100%;">
              <p>üìÅ Drag &amp; drop files (10Mb max each)</p>
              <label class="file-input-label" style="width: 100%; text-align: center;">
                Select Files <input id="folderInput" webkitdirectory="" multiple=""
                  accept="audio/wav,audio/mpeg"
                  type="file">
              </label> </div>
          </div>
          <!-- Recording Input -->
          <div class="control-section" style="display: flex; flex-direction: column; align-items: center; width: 100%; max-width: 400px;">
            <h3>Recording Input</h3>
            <button id="micBtn" class="secondary" style="width: 100%;">
              <svg width="16" height="16" fill="currentColor" viewBox="0 0 24 24"><path
                  d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3zm5.91-3c-.49 0-.9.36-.98.85C16.52 14.2 14.47 16 12 16s-4.52-1.8-4.93-4.15c-.08-.49-.49-.85-.98-.85-.61 0-1.09.54-1 1.14.49 3 2.89 5.35 5.91 5.78V20c0 .55.45 1 1 1s1-.45 1-1v-2.08c3.02-.43 5.42-2.78 5.91-5.78.1-.6-.39-1.14-1-1.14z"></path></svg>
              Record Live </button>
            <p id="micResult" class="status-indicator" style="width: 100%; text-align: center;">Click
              to start</p>
          </div>
          <!-- File Input -->
          <div class="control-section" style="display: flex; flex-direction: column; align-items: center; width: 100%; max-width: 400px;">
            <h3>File Input</h3>
            <label class="file-input-label" style="width: 100%; text-align: center;">
              Load Audio File <input id="singleFileInput" accept="audio/wav,audio/mpeg"
                type="file">
            </label> <button id="analyzeSingleBtn" class="secondary" style="width: 100%;">Analyze</button>
            <div id="analysis-progress">Analysis Progress: 0%</div>
          </div>
          <!-- Real-Time Analysis -->
          <div class="control-section" style="display: flex; flex-direction: column; align-items: center; width: 100%; max-width: 400px;">
            <h3>Real-Time Analysis</h3>
            <div style="display: flex; gap: 8px; align-items: center; justify-content: center; width: 100%;">
              <input id="realTimeDuration" min="1" max="1440" value="1" style="width: 60px; padding: 5px; background: var(--panel-bg); border: 1px solid var(--nature); color: white; border-radius: 4px;"
                type="number">
              <span>min (max: 1440 min / 24 hours)</span> </div>
            <button id="startRealTimeBtn" class="secondary" style="width: 100%;">Start</button>
            <button id="stopRealTimeBtn" class="secondary" style="width: 100%; display: none;">Stop</button>
            <p id="realTimeStatus" class="status-indicator" style="width: 100%; text-align: center;">Enter
              duration &amp; click Start</p>
            <button id="downloadFullRecordingBtn" class="secondary" style="width: 100%; display: none;"
              disabled="disabled">Download
              Full</button> </div>
        </div>
      </div>
          <p id="status" class="status-indicator">Ready to analyze audio files.</p>
      <!-- Enhanced Training Panel -->
      <div class="panel">
        <h2 class="panel-title">
          <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"
            fill="white">
            <path d="M12 3v10.55c-.59-.34-1.27-.55-2-.55-2.21 0-4 1.79-4 4s1.79 4 4 4 4-1.79 4-4V7h4V3h-6z"></path>
          </svg> Train Custom Sounds </h2>
        <div class="training-controls">
          <!-- 1.  ADD THIS -->
          <div id="trainingStatus" class="status-indicator">Ready to train new
            sounds</div>
          <div class="training-progress" style="display:none"> <progress value="0"
              max="100"></progress>
            <span class="progress-text">0%</span> </div>
          <div id="trainingResults" class="training-results"></div>
          <label class="file-input-label">
            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"
              fill="currentColor">
              <path d="M19 13h-6v6h-2v-6H5v-2h6V5h2v6h6v2z"></path> </svg>
            Select Training Samples (5+ recommended) <input id="trainingSamplesInput"
              multiple=""
              accept="audio/wav,audio/mpeg"
              type="file">
          </label>
          <div class="input-group"> <input id="newCategoryName" placeholder="New category name (e.g. 'Coffee Machine')"
              type="text">
            <button id="startTrainingBtn" class="secondary">
              <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"
                fill="white">
                <path d="M12 3v10.55c-.59-.34-1.27-.55-2-.55-2.21 0-4 1.79-4 4s1.79 4 4 4 4-1.79 4-4V7h4V3h-6z"></path>
              </svg> Train </button> </div>
          <div class="control-section">
            <h3>Add Negative Examples</h3>
            <label class="file-input-label">
              <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"
                fill="currentColor">
                <path d="M19 13h-6v6h-2v-6H5v-2h6V5h2v6h6v2z"></path> </svg>
              Select Negative Samples <input id="negativeSamplesInput" multiple=""
                accept="audio/wav,audio/mpeg"
                type="file">
            </label> <button id="addNegativeBtn" class="secondary">
              <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"
                fill="white">
                <path d="M12 3v10.55c-.59-.34-1.27-.55-2-.55-2.21 0-4 1.79-4 4s1.79 4 4 4 4-1.79 4-4V7h4V3h-6z"></path>
              </svg> Add Negatives </button> </div>
          <button id="saveModelBtn">
            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"
              fill="currentColor">
              <path d="M17 3H5c-1.11 0-2 .9-2 2v14c0 1.1.89 2 2 2h14c1.1 0 2-.9 2-2V7l-4-4zm-5 16c-1.66 0-3-1.34-3-3s1.34-3 3-3 3 1.34 3 3-1.34 3-3 3zm3-10H5V5h10v4z"></path>
            </svg> Save Model </button> <button id="loadModelBtn" class="secondary">
            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"
              fill="white">
              <path d="M19 9h-4V3H9v6H5l7 7 7-7z"></path> </svg> Load Model </button>
        </div>
      </div>
      <!-- keep this one too -->
      <ul id="customCategoriesList" style="margin:0 0 10px 0; padding-left:20px; color:#e8c547;">
      </ul>
    </div>
    <div class="panel">
      <h2 class="panel-title">
        <svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor">
          <path d="M8 5v14l11-7z"></path> </svg> Audio Render (Folder Method
        Only) </h2>
      <div class="control-section">
        <!-- NEW CONTROLS -->
        <div class="slider-container"> <label>Playback mode</label>
          <select id="renderMode">
            <option value="low‚Üíhigh">low ‚Üí high frequency</option>
            <option value="high‚Üílow">high ‚Üí low frequency</option>
            <option value="lorenz">Lorenz attractor</option>
            <option value="markov">Markov chain</option>
            <option value="random">random mix</option>
            <option value="alt">alternate natural / man-made</option>
          </select>
        </div>
        <div class="slider-container">
          <div class="slider-label"><span>Time-stretch:</span> <span id="stretchValue">1</span>x</div>
          <input id="stretchSlider" min="0.01" max="1" step="0.01" value="1" type="range">
        </div>
   <div class="slider-container">
  <div class="slider-label"><span>Render length (s) (max 600 sec / 10 minutes):</span> <span id="lenValue">60</span>s</div>
  <input id="lenSlider" min="1" max="600" step="1" value="60" type="range">
</div>
        
        <!-- ORIGINAL CONTROLS --> <button id="renderBtn">
          <svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor">
            <path d="M8 5v14l11-7z"></path> </svg> Render Combined </button>
        <div class="button-group"> <button id="playBtn" disabled="disabled">‚ñ∂
            Play</button> <button id="downloadBtn" class="secondary" disabled="disabled">‚¨á
            Download</button> </div>
        <div class="slider-container">
          <div class="slider-label"><span>Fade:</span> <span id="fadeValue">1</span>s</div>
          <input min="0" max="5" step="0.1" value="1" id="fadeSlider" type="range">
        </div>
        <div class="slider-container">
          <div class="slider-label"><span>Speed:</span> <span id="speedValue">1</span>x</div>
          <input min="0.5" max="2" step="0.1" value="1" id="speedSlider" type="range">
        </div>
            <!-- Combined Audio Section -->
    <div class="panel">
      <h2 class="panel-title">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
          <path d="M12 3v10.55c-.59-.34-1.27-.55-2-.55-2.21 0-4 1.79-4 4s1.79 4 4 4 4-1.79 4-4V7h4V3h-6z"></path>
        </svg> Combined Audio </h2>
      <div id="combinedWaveform" class="waveform"></div>
    </div>
      </div>
    </div>
    <!-- === MIDI MAPPER === -->
    <div class="panel">
      <h2 class="panel-title">üéπ MIDI Mapper</h2>
      <label>MIDI Port
        <select id="midiPortSelect">
        </select>
      </label>
      <div id="midiRows"></div>
      <button id="addMidiRowBtn" class="secondary">+ Add Mapping</button> <button
        id="checkMidiBtn"
        class="secondary">Check
        MIDI Output</button> </div>
    <!-- Pie Charts Section -->
    <div class="panel pie-container">
      <h2 class="panel-title">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
          <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm7.93 9H13V4.07c3.61.45 6.48 3.32 6.93 6.93zM4 12c0-4.07 3.06-7.44 7-7.93v15.86c-3.94-.49-7-3.86-7-7.93zm9 7.93V13h6.93c-.45 3.61-3.32 6.48-6.93 6.93z"></path>
        </svg> Soundscape Fidelity </h2>
      <canvas id="pieChart" class="mx-auto" width="260" height="260"></canvas> <canvas
        id="bgPieChart"
        class="mx-auto mt-4"
        width="260"
        height="260"></canvas>
      <p id="dailyReport" class="status-indicator">No data yet. Record or upload
        audio.</p>
    </div>
    <!-- Analysis Results Section -->
    <div class="panel">
      <h2 class="panel-title">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
          <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"></path>
        </svg> Folder Analysis Results </h2>
      <button onclick="updatePieWith('folder')">üçï Update Pie with these results</button>
      <table id="fileTable">
        <thead>
          <tr>
            <th>File</th>
            <th>Waveform</th>
            <th>Top-3 Labels (Score)</th>
            <th>Category</th>
            <th>Confidence Over Time</th>
          </tr>
        </thead>
        <tbody>
        </tbody>
      </table>
    </div>
    <!-- Single Recording Analysis Section -->
    <div class="panel">
      <h2 class="panel-title">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
          <path d="M12 3v10.55c-.59-.34-1.27-.55-2-.55-2.21 0-4 1.79-4 4s1.79 4 4 4 4-1.79 4-4V7h4V3h-6z"></path>
        </svg> Single File Recording Analysis </h2>
      <button onclick="updatePieWith('single')">üçï Update Pie with these results</button>
      <table id="singleFileTable">
        <thead>
          <tr>
            <th>Time (s)</th>
            <th>Top Label (Score)</th>
            <th>Category</th>
            <th>Download</th>
          </tr>
        </thead>
        <tbody>
        </tbody>
      </table>
    </div>
    <!-- Real-Time Analysis Table -->
    <div class="panel">
      <h2 class="panel-title">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
          <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-1 14H9V8h2v8zm4 0h-2V8h2v8z"></path>
        </svg> Real-Time Analysis </h2>
      <button onclick="updatePieWith('realtime')">üçï Update Pie with these
        results</button>
      <table id="realTimeTable">
        <thead>
          <tr>
            <th>Time (s)</th>
            <th>Top Label (Score)</th>
            <th>Category</th>
            <th>Download</th>
          </tr>
        </thead>
        <tbody>
        </tbody>
      </table>
    </div>

    <!-- Libraries -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/wavesurfer.js@7/dist/wavesurfer.min.js"></script>
    <script type="module">
  import { AudioClassifier, AudioEmbedder, FilesetResolver }
  from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-audio@latest/audio_bundle.js";
  
    let audioClassifier;
    let audioCtx;
    let pieChart;
    let naturalCount = 0;
    let manmadeCount = 0;
    let naturalChallengeProgress = 0;
    let analyzedFiles = [];
    let singleRecordingSegments = [];
    let realTimeSegments = [];
    let fadeDuration = 1;
    let playbackSpeed = 1;
    let combinedBuffer = null;
    let combinedWaveformInstance = null;
    let realTimeRecording = null;
    let realTimeTimeout = null;
    let segmentSource = null;   // keeps track of the last source so we can stop it
    let bgNaturalCount = 0;
let bgManmadeCount = 0;
let fullRealtimeBuffer = null;
let customClassifier = null;
let customCategories = new Set();
let trainingSamples = [];
let featureExtractor = null


    const NATURAL = new Set([
      "Bird", "Animals", "Wildlife", "Wind", "Rain", "Thunderstorm", "Water", "Ocean", "Sea waves",
      "Insect", "Cricket", "Frog", "River", "Stream", "Leaves", "Forest", "Fire", "Crackling fire",
      "Rain on surface", "Waterfall", "Babbling brook", "Bird vocalization", "Bird chirping",
      "Cicada", "Crickets", "Frog croaking", "Tree", "Jungle", "Nature", "Ambient nature"
    ]);

    const SUPPORTED_AUDIO_TYPES = ['audio/wav', 'audio/mpeg'];

    const bucket = label => NATURAL.has(label) ? "Natural" : "Man-made";
    const baseName = path => path.split("/").pop();
    
    function playSegment(segment) {
  if (!segment.audioBuffer) return;
  if (segmentSource) segmentSource.stop();      // stop previous sound
  if (audioCtx?.state === 'suspended') audioCtx.resume();
  segmentSource = audioCtx.createBufferSource();
  segmentSource.buffer = segment.audioBuffer;
  segmentSource.connect(audioCtx.destination);
  segmentSource.start();
}

    async function initializeAudioClassifier() {
      try {
        console.log("Initializing audio classifier...");
        const audio = await FilesetResolver.forAudioTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-audio@latest/wasm"
        );
        audioClassifier = await AudioClassifier.createFromOptions(audio, {
          baseOptions: {
            modelAssetPath: "https://storage.googleapis.com/mediapipe-models/audio_classifier/yamnet/float32/1/yamnet.tflite"
          },
          maxResults: 3,
          scoreThreshold: 0.1
        });
        console.log("Audio classifier initialized successfully");
        updateStatus("Model loaded. Upload Sample Folder or a Single File or make a Realtime Analysis.");
            await initializeFeatureExtractor();
            
      } catch (error) {
        console.error("Failed to initialize audio classifier:", error);
        updateStatus("Error loading model. Retrying in 5 seconds...", true);
        setTimeout(initializeAudioClassifier, 5000);
      }
    }
    


let embedder;   // will hold the AudioEmbedder instance

async function initializeFeatureExtractor() {
  try {
    console.log("Loading AudioEmbedder‚Ä¶");
    const audio = await FilesetResolver.forAudioTasks(
      "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-audio@latest/wasm"
    );

    embedder = await AudioEmbedder.createFromOptions(audio, {
      baseOptions: {
        // Same YAMNet model, but we‚Äôll use its embeddings
        modelAssetPath:
          "https://storage.googleapis.com/mediapipe-models/audio_classifier/yamnet/float32/1/yamnet.tflite"
      }
    });

    console.log("AudioEmbedder ready");
    updateTrainingStatus("Feature extractor loaded ‚úî");
  } catch (err) {
    console.error("AudioEmbedder failed:", err);
    updateTrainingStatus("Error loading embedder: " + err.message, true);
  }
}


// ===== TRAINING FUNCTIONS =====
function updateTrainingStatus(message, isError = false) {
  const statusEl = document.getElementById('trainingStatus');
  statusEl.textContent = message;
  statusEl.style.color = isError ? 'red' : 'white';
}



// Enhanced classify function to handle embeddings
async function enhancedClassify(pcm, sampleRate) {
  if (!pcm || pcm.length === 0) {
    console.error("Invalid PCM data: empty or null");
    return [];
  }
  if (!audioClassifier) {
    console.error("Audio classifier not initialized");
    return [];
  }
  try {
    const yamnetResults = await audioClassifier.classify(pcm, sampleRate);
    if (!featureExtractor) {
      console.warn("Feature extractor not initialized, skipping custom classification");
      return yamnetResults;
    }
    const embeddingResults = await featureExtractor.classify(pcm, sampleRate);
    if (!embeddingResults?.[0]?.embeddings?.[0]?.floatEmbedding) {
      console.warn("No embeddings returned from feature extractor");
      return yamnetResults;
    }
    const embedding = embeddingResults[0].embeddings[0].floatEmbedding;
    
    // Custom KNN classification
    if (customClassifier) {
      let bestCategory = null;
      let bestDistance = Infinity;
      for (const category in customClassifier.embeddings) {
        for (const trainedEmbedding of customClassifier.embeddings[category]) {
          let distance = 0;
          for (let i = 0; i < embedding.length; i++) {
            distance += Math.pow(embedding[i] - trainedEmbedding[i], 2);
          }
          distance = Math.sqrt(distance);
          if (distance < bestDistance) {
            bestDistance = distance;
            bestCategory = category;
          }
        }
      }
      if (bestCategory && bestDistance < 1.0) {
        yamnetResults[0].classifications[0].categories.unshift({
          categoryName: bestCategory,
          score: Math.max(0, 1 - bestDistance),
          index: -1
        });
      }
    }
    return yamnetResults;
  } catch (err) {
    console.error("Error in enhancedClassify:", err);
    return [];
  }
} 
    
    // call this inside processSegments and analyseSingleRecording
function classifyBackground(segments) {
  // reset counters first
  bgNaturalCount = 0;
  bgManmadeCount = 0;

  // simple rule: count only segments whose label appears ‚â• 3 times in a row
  let run = [];
  for (const s of segments) {
    run.push(s);
    if (run.length >= 3 && run.every(r => r.topLabel === run[0].topLabel)) {
      const cat = bucket(run[0].topLabel);
      cat === "Natural" ? bgNaturalCount++ : bgManmadeCount++;
      run = [];   // consume the run
    }
  }
}

function drawBackgroundPie() {
  const ctx = document.getElementById("bgPieChart");
  if (window.bgPieInstance) bgPieInstance.destroy();
  const total = bgNaturalCount + bgManmadeCount;
  window.bgPieInstance = new Chart(ctx, {
    type: "pie",
    data: {
      labels: ["Background-Natural", "Background-Man-made"],
      datasets: [{
        data: [bgNaturalCount, bgManmadeCount],
        backgroundColor: ["#4CAF50", "#F44336"]
      }]
    },
    options: { responsive: false }
  });
}
    function encodeWAV(audioBuffer) {
  const length = audioBuffer.length * audioBuffer.numberOfChannels * 2 + 44;
  const buffer = new ArrayBuffer(length);
  const view = new DataView(buffer);

  const writeString = (offset, str) => {
    for (let i = 0; i < str.length; i++) view.setUint8(offset + i, str.charCodeAt(i));
  };

  writeString(0, 'RIFF');
  view.setUint32(4, length - 8, true);
  writeString(8, 'WAVE');

  writeString(12, 'fmt ');
  view.setUint32(16, 16, true);
  view.setUint16(20, 1, true);                       // PCM
  view.setUint16(22, audioBuffer.numberOfChannels, true);
  view.setUint32(24, audioBuffer.sampleRate, true);
  view.setUint32(28, audioBuffer.sampleRate * audioBuffer.numberOfChannels * 2, true);
  view.setUint16(32, audioBuffer.numberOfChannels * 2, true);
  view.setUint16(34, 16, true);                      // bits per sample
  writeString(36, 'data');
  view.setUint32(40, length - 44, true);

  const channels = [];
  for (let c = 0; c < audioBuffer.numberOfChannels; c++)
    channels.push(audioBuffer.getChannelData(c));

  let offset = 44;
  for (let i = 0; i < audioBuffer.length; i++) {
    for (let c = 0; c < audioBuffer.numberOfChannels; c++) {
      const sample = Math.max(-1, Math.min(1, channels[c][i]));
      view.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true);
      offset += 2;
    }
  }
  return buffer;
}

    function updateStatus(message, isError = false) {
      console.log(`Status: ${message}`);
      document.getElementById("status").textContent = message;
      document.getElementById("status").style.color = isError ? "red" : "white";
    }

    function drawPie(naturalCount, manmadeCount) {
      console.log(`Drawing pie chart: Natural=${naturalCount}, Man-made=${manmadeCount}`);
      const ctx = document.getElementById("pieChart");
      if (pieChart) pieChart.destroy();
      const total = naturalCount + manmadeCount;
      const naturalPercent = total ? ((naturalCount / total) * 100).toFixed(0) : 0;
      document.getElementById("dailyReport").textContent = total ?
        `You heard ${naturalPercent}% natural sounds today` :
        "No data yet. Record or upload audio.";
      pieChart = new Chart(ctx, {
        type: "pie",
        data: {
          labels: ["Natural", "Man-made"],
          datasets: [{ 
            data: [naturalCount, manmadeCount], 
            backgroundColor: ["#4CAF50", "#F44336"] 
          }]
        },
        options: { responsive: false }
      });
    }

    function createWaveformCanvas(audioBuffer) {
      console.log("Creating waveform for audio buffer");
      const container = document.createElement("div");
      container.className = "waveform";
      const wavesurfer = WaveSurfer.create({
        container: container,
        cursorWidth: 0,
        barWidth: 1,
        height: 60,
        waveColor: '#888',
        progressColor: '#555',
        backend: 'WebAudio'
      });
const wav = encodeWAV(audioBuffer);   // <-- uses the helper below
const blob = new Blob([wav], { type: 'audio/wav' });
      console.log("Loading waveform blob");
      wavesurfer.loadBlob(blob);
      wavesurfer.on('error', (err) => {
        console.error('WaveSurfer error:', err);
        updateStatus(`Failed to load waveform: ${err.message}`, true);
      });
      wavesurfer.on('ready', () => {
        console.log("Waveform ready");
        if (container === document.getElementById("combinedWaveform").querySelector('.waveform')) {
          document.getElementById("playBtn").disabled = false;
          updateStatus("Combined audio ready to play!");
        }
      });
      return { container, wavesurfer };
    }

    function createTimeChart(categoriesOverTime) {
      console.log("Creating time chart");
      const container = document.createElement("div");
      container.className = "time-chart";
      const canvas = document.createElement("canvas");
      container.appendChild(canvas);
      new Chart(canvas, {
        type: 'line',
        data: {
          labels: categoriesOverTime.map((_, i) => i),
          datasets: [
            {
              label: 'Natural Confidence',
              data: categoriesOverTime.map(r => r.naturalScore),
              borderColor: '#4CAF50',
              backgroundColor: 'rgba(76, 175, 80, 0.1)',
              tension: 0.1,
              fill: true
            },
            {
              label: 'Man-made Confidence',
              data: categoriesOverTime.map(r => r.manmadeScore),
              borderColor: '#F44336',
              backgroundColor: 'rgba(244, 67, 54, 0.1)',
              tension: 0.1,
              fill: true
            }
          ]
        },
        options: {
          responsive: true,
          scales: { y: { min: 0, max: 1 } }
        }
      });
      return container;
    }

    async function estimateBaseFrequency(audioBuffer) {
      console.log("Estimating base frequency");
      try {
        const audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
        const analyser = audioContext.createAnalyser();
        analyser.fftSize = 2048;
        const bufferLength = analyser.frequencyBinCount;
        const dataArray = new Float32Array(bufferLength);
        const source = audioContext.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(analyser);
        analyser.getFloatFrequencyData(dataArray);
        let maxIndex = 0;
        let maxValue = -Infinity;
        for (let i = 0; i < bufferLength; i++) {
          if (dataArray[i] > maxValue) {
            maxValue = dataArray[i];
            maxIndex = i;
          }
        }
        const frequency = (maxIndex * audioContext.sampleRate) / analyser.fftSize;
        source.disconnect();
        audioContext.close();
        console.log(`Base frequency: ${frequency} Hz`);
        return frequency;
      } catch (err) {
        console.error("Error estimating frequency:", err);
        throw new Error(`Failed to estimate frequency: ${err.message}`);
      }
    }
    
    function updatePieWith(src) {
  let list = src === 'folder' ? analyzedFiles
           : src === 'single' ? singleRecordingSegments
           : realTimeSegments;
  updateGlobalPieWith(list);
}

    function bufferToWave(abuffer, len) {
      console.log(`Converting buffer to WAV, length: ${len}`);
      try {
        const numOfChan = abuffer.numberOfChannels;
        const length = len * numOfChan * 2 + 44;
        const buffer = new ArrayBuffer(length);
        const view = new DataView(buffer);
        const channels = [];

        function writeString(view, offset, string) {
          for (let i = 0; i < string.length; i++) {
            view.setUint8(offset + i, string.charCodeAt(i));
          }
        }

        let offset = 0;
        writeString(view, offset, 'RIFF'); offset += 4;
        view.setUint32(offset, 36 + len * numOfChan * 2, true); offset += 4;
        writeString(view, offset, 'WAVE'); offset += 4;
        writeString(view, offset, 'fmt '); offset += 4;
        view.setUint32(offset, 16, true); offset += 4;
        view.setUint16(offset, 1, true); offset += 2;
        view.setUint16(offset, numOfChan, true); offset += 2;
        view.setUint32(offset, abuffer.sampleRate, true); offset += 4;
        view.setUint32(offset, abuffer.sampleRate * numOfChan * 2, true); offset += 4;
        view.setUint16(offset, numOfChan * 2, true); offset += 2;
        view.setUint16(offset, 16, true); offset += 2;
        writeString(view, offset, 'data'); offset += 4;
        view.setUint32(offset, len * numOfChan * 2, true); offset += 4;

        for (let i = 0; i < abuffer.numberOfChannels; i++) {
          channels.push(abuffer.getChannelData(i));
        }

        for (let i = 0; i < len; i++) {
          for (let channel = 0; channel < numOfChan; channel++) {
            const sample = Math.max(-1, Math.min(1, channels[channel][i]));
            view.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true);
            offset += 2;
          }
        }

        return buffer;
      } catch (err) {
        console.error("Error converting buffer to WAV:", err);
        throw new Error(`Failed to convert to WAV: ${err.message}`);
      }
    }

 function downloadCombinedAudio() {
  if (!combinedBuffer) {
    updateStatus("No combined audio available. Please render first.", true);
    return;
  }

  // Build a correct WAV Blob
  const length = combinedBuffer.length * combinedBuffer.numberOfChannels * 2 + 44;
  const arrayBuffer = new ArrayBuffer(length);
  const view = new DataView(arrayBuffer);
  const channels = [];
  for (let c = 0; c < combinedBuffer.numberOfChannels; c++) {
    channels.push(combinedBuffer.getChannelData(c));
  }

  // Helper to write strings
  const writeString = (offset, s) => {
    for (let i = 0; i < s.length; i++) view.setUint8(offset + i, s.charCodeAt(i));
  };

  /* RIFF header */
  writeString(0, 'RIFF');
  view.setUint32(4, length - 8, true);
  writeString(8, 'WAVE');

  /* fmt chunk */
  writeString(12, 'fmt ');
  view.setUint32(16, 16, true);           // sub-chunk size
  view.setUint16(20, 1, true);            // PCM
  view.setUint16(22, combinedBuffer.numberOfChannels, true);
  view.setUint32(24, combinedBuffer.sampleRate, true);
  view.setUint32(28, combinedBuffer.sampleRate * combinedBuffer.numberOfChannels * 2, true);
  view.setUint16(32, combinedBuffer.numberOfChannels * 2, true);
  view.setUint16(34, 16, true);           // bits per sample

  /* data chunk */
  writeString(36, 'data');
  view.setUint32(40, length - 44, true);

  /* interleave samples */
  let offset = 44;
  const volume = 1;
  for (let i = 0; i < combinedBuffer.length; i++) {
    for (let c = 0; c < combinedBuffer.numberOfChannels; c++) {
      const sample = Math.max(-1, Math.min(1, channels[c][i]));
      view.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true);
      offset += 2;
    }
  }

  const blob = new Blob([arrayBuffer], { type: 'audio/wav' });
  const url = URL.createObjectURL(blob);
  const a = document.createElement('a');
  a.href = url;
  a.download = 'sound_diet_combined.wav';
  a.click();
  URL.revokeObjectURL(url);
  updateStatus("Combined audio downloaded successfully!");
}

    function downloadSegment(segment, filename) {
      console.log(`Downloading segment: ${filename}`);
      try {
        if (!segment.audioBuffer) {
          throw new Error("No audio data available for this segment");
        }
        const wavBuffer = bufferToWave(segment.audioBuffer, segment.audioBuffer.length);
        const blob = new Blob([wavBuffer], { type: 'audio/wav' });
        const url = URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.href = url;
        a.download = filename;
        document.body.appendChild(a);
        a.click();
        document.body.removeChild(a);
        URL.revokeObjectURL(url);
        console.log(`Segment downloaded: ${filename}`);
        updateStatus(`Downloaded segment: ${filename}`);
      } catch (err) {
        console.error(`Error downloading segment:`, err);
        updateStatus(`Failed to download segment: ${err.message}`, true);
      }
    }


// Add this function to handle combined rendering for real-time and single files
function renderCombined() {
  if (!audioCtx || audioCtx.state === 'closed') {
    audioCtx = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
  }

  // Get segments from both real-time and single file analyses
  const segments = [...realTimeSegments, ...singleRecordingSegments].filter(s => s.audioBuffer);
  
  if (segments.length === 0) {
    updateStatus("No audio segments available to render", true);
    return;
  }

  // Calculate total duration
  const fade = parseFloat(document.getElementById('fadeSlider').value);
  const targetDuration = parseInt(document.getElementById('lenSlider').value);
  const totalDuration = segments.reduce((sum, s) => sum + s.audioBuffer.duration, 0);
  const loops = Math.ceil(targetDuration / totalDuration);
  
  // Create output buffer
  const outputBuffer = audioCtx.createBuffer(
    1, 
    Math.ceil(targetDuration * audioCtx.sampleRate), 
    audioCtx.sampleRate
  );
  const outputData = outputBuffer.getChannelData(0);
  
  // Render segments with crossfades
  let offset = 0;
  for (let l = 0; l < loops; l++) {
    for (const segment of segments) {
      const inputBuffer = segment.audioBuffer;
      const inputData = inputBuffer.getChannelData(0);
      const fadeSamples = Math.min(
        Math.floor(fade * audioCtx.sampleRate),
        Math.floor(inputBuffer.length / 2)
      );
      
      // Copy with fade in/out
      for (let i = 0; i < inputBuffer.length && offset < outputData.length; i++) {
        let gain = 1;
        if (i < fadeSamples) gain = i / fadeSamples; // fade in
        else if (i > inputBuffer.length - fadeSamples) gain = (inputBuffer.length - i) / fadeSamples; // fade out
        
        outputData[offset] = (outputData[offset] || 0) + (inputData[i] * gain);
        offset++;
      }
    }
  }

  // Update UI with the rendered audio
  combinedBuffer = outputBuffer;
  document.getElementById("combinedWaveform").innerHTML = "";
  const { container, wavesurfer } = createWaveformCanvas(outputBuffer);
  document.getElementById("combinedWaveform").appendChild(container);
  combinedWaveformInstance = wavesurfer;
  
  document.getElementById("playBtn").disabled = false;
  document.getElementById("downloadBtn").disabled = false;
  updateStatus(`Rendered ${segments.length} segments (${(outputBuffer.duration).toFixed(1)}s)`);
}

// Add this event listener to trigger the rendering
document.getElementById('renderBtn').addEventListener('click', function() {
  // Check if we have real-time or single file segments to render
  if (realTimeSegments.length > 0 || singleRecordingSegments.length > 0) {
    renderCombined();
  } else if (analyzedFiles.length > 0) {
    // Fall back to original folder-based rendering
    combineAudioFiles();
  } else {
    updateStatus("No audio available to render", true);
  }
});


    async function combineAudioFiles() {
      if (!analyzedFiles.length) {
        updateStatus("No audio files to combine.", true);
        console.warn("No analyzed files available");
        return;
      }
      updateStatus("Combining audio files...");
      console.log("Starting combineAudioFiles");
      try {
        if (!audioCtx || audioCtx.state === "closed") {
          audioCtx = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
          console.log("Initialized new AudioContext");
        }
        console.log("Sorting files by frequency");
        const validFiles = analyzedFiles.filter(f => f.audioBuffer);
        if (!validFiles.length) {
          updateStatus("No valid audio files to combine.", true);
          console.warn("No valid audio buffers available");
          return;
        }
        const sortedFiles = await Promise.all(validFiles.map(async (file, index) => {
          try {
            const frequency = await estimateBaseFrequency(file.audioBuffer);
            return { ...file, frequency };
          } catch (err) {
            console.error(`Failed to estimate frequency for file ${index} (${baseName(file.file.name)}):`, err);
            updateStatus(`Skipping ${baseName(file.file.name)}: ${err.message}`, true);
            return null;
          }
        }));
        const orderedFiles = sortedFiles.filter(f => f !== null);
        console.log(`Ordered files: ${orderedFiles.length}`);
        if (!orderedFiles.length) {
          updateStatus("No valid audio files to combine after frequency estimation.", true);
          return;
        }
        const manmade = orderedFiles
          .filter(f => f.category === "Man-made")
          .sort((a, b) => a.frequency - b.frequency);
        const natural = orderedFiles
          .filter(f => f.category === "Natural")
          .sort((a, b) => b.frequency - a.frequency);
        const finalFiles = [...manmade, ...natural];
        console.log(`Final files to combine: ${finalFiles.length}`);
        if (!finalFiles.length) {
          updateStatus("No valid audio files to combine.", true);
          return;
        }
        const channels = finalFiles[0].audioBuffer.numberOfChannels;
        console.log(`Using ${channels} channels`);
        for (const file of finalFiles) {
          if (file.audioBuffer.numberOfChannels !== channels) {
            updateStatus(`Skipping combination: Channel mismatch in ${baseName(file.file.name)} (has ${file.audioBuffer.numberOfChannels}, expected ${channels})`, true);
            console.error(`Channel mismatch for ${baseName(file.file.name)}`);
            return;
          }
        }
        let totalDuration = 0;
        finalFiles.forEach(f => totalDuration += f.audioBuffer.duration - fadeDuration);
        totalDuration += fadeDuration;
        console.log(`Total duration: ${totalDuration} seconds`);
        combinedBuffer = audioCtx.createBuffer(
          channels,
          totalDuration * audioCtx.sampleRate,
          audioCtx.sampleRate
        );
        console.log("Created combined buffer");
        let offset = 0;
        for (let i = 0; i < finalFiles.length; i++) {
          const buffer = finalFiles[i].audioBuffer;
          const duration = buffer.duration;
          console.log(`Processing file ${i + 1}/${finalFiles.length}: ${baseName(finalFiles[i].file.name)}`);
          for (let channel = 0; channel < channels; channel++) {
            const inputData = buffer.getChannelData(channel);
            const outputData = combinedBuffer.getChannelData(channel);
            const fadeSamples = fadeDuration * audioCtx.sampleRate;
            for (let j = 0; j < inputData.length; j++) {
              let gain = 1;
              if (j < fadeSamples) {
                gain = j / fadeSamples; // Fade in
              } else if (j > inputData.length - fadeSamples) {
                gain = (inputData.length - j) / fadeSamples; // Fade out
              }
              outputData[offset + j] += inputData[j] * gain;
            }
          }
          offset += (duration - fadeDuration) * audioCtx.sampleRate;
        }
        console.log("Rendering combined waveform");
        const combinedWaveform = document.getElementById("combinedWaveform");
        combinedWaveform.innerHTML = "";
        const { container, wavesurfer } = createWaveformCanvas(combinedBuffer);
        combinedWaveform.appendChild(container);
        wavesurfer.setPlaybackRate(playbackSpeed);
        combinedWaveformInstance = wavesurfer;
        document.getElementById("playBtn").disabled = false;
        combinedWaveform.wavesurfer = wavesurfer;
        combinedWaveformInstance = wavesurfer;
        wavesurfer.on('play', () => {
          console.log("Waveform playback started");
          document.getElementById("playBtn").textContent = "‚è∏ Pause";
        });
        wavesurfer.on('pause', () => {
          console.log("Waveform playback paused");
          document.getElementById("playBtn").textContent = "‚ñ∂ Play";
        });
        wavesurfer.on('finish', () => {
          console.log("Waveform playback finished");
          document.getElementById("playBtn").textContent = "‚ñ∂ Play";
        });
        document.getElementById("downloadBtn").disabled = false;
        updateStatus("Combined audio rendered successfully!");
        return combinedBuffer;
      } catch (err) {
        console.error("Error combining audio files:", err);
        updateStatus(`Failed to combine audio: ${err.message}`, true);
        return null;
      }
    }

    async function decodeFile(file) {
      console.log(`Decoding file: ${file.name}, Type: ${file.type}, Size: ${file.size} bytes`);
      try {
        if (!SUPPORTED_AUDIO_TYPES.includes(file.type)) {
          throw new Error(`Unsupported audio format: ${file.type}. Please use WAV or MP3.`);
        }
        if (file.size === 0) {
          throw new Error(`File is empty: ${file.name}`);
        }
        if (file.size > 10 * 1024 * 1024) { // Limit to 10MB
          throw new Error(`File too large: ${file.name} (${(file.size / 1024 / 1024).toFixed(1)}MB). Max 10MB.`);
        }
        const buffer = await file.arrayBuffer();
        if (!audioCtx) {
          audioCtx = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
        }
        try {
          const audioBuffer = await audioCtx.decodeAudioData(buffer);
          const mono = audioCtx.createBuffer(1, audioBuffer.length, audioBuffer.sampleRate);
mono.getChannelData(0).set(
  audioBuffer.numberOfChannels === 1
    ? audioBuffer.getChannelData(0)
    : audioBuffer.getChannelData(0).map((v, i) =>
        v + (audioBuffer.getChannelData(1)?.[i] || 0) * 0.5
      )
);
return mono;
          console.log(`Successfully decoded: ${file.name}`);
          return audioBuffer;
        } catch (decodeErr) {
          console.error(`Decode error for ${file.name}:`, decodeErr);
          throw new Error(`Unable to decode audio data. File may be corrupted or unsupported.`);
        }
      } catch (err) {
        console.error(`Failed to decode: ${file.name}`, err);
        throw new Error(`Unable to process ${file.name}: ${err.message}`);
      }
    }

    async function processAudioFile(file, audioBuffer) {
      console.log(`Processing audio file: ${file.name}`);
      if (!audioClassifier) {
        throw new Error("Audio classifier not initialized");
      }
      const pcm = audioBuffer.getChannelData(0);
      try {
     const results = await enhancedClassify(pcm, audioCtx.sampleRate);
        if (!results?.[0]?.classifications?.[0]?.categories) {
          throw new Error("No classification results");
        }
        console.log(`Classification results for ${file.name}:`, results);
        const categoriesOverTime = [];
        if (results.length > 1) {
          for (const result of results) {
            const cats = result.classifications[0].categories;
            const naturalEntry = cats.find(c => NATURAL.has(c.categoryName));
            const manmadeEntry = cats.find(c => !NATURAL.has(c.categoryName));
            categoriesOverTime.push({
              naturalScore: naturalEntry?.score || 0,
              manmadeScore: manmadeEntry?.score || 0,
              timestamp: result.timestampMs
            });
          }
        }
        const topCategory = results[0].classifications[0].categories[0];
        const category = bucket(topCategory.categoryName);
        if (category === "Natural" && topCategory.score >= 0.8) {
          naturalChallengeProgress++;
          document.getElementById("naturalChallengeProgress").textContent = `${naturalChallengeProgress}/5`;
          localStorage.setItem("naturalChallengeProgress", naturalChallengeProgress);
        }
        analyzedFiles.push({ file, audioBuffer, category, topCategories: results[0].classifications[0].categories });
        console.log(`Processed ${file.name} as ${category}`);
            // Send MIDI message
    sendMIDI(topCategory.categoryName, topCategory.score);
        return {
          topCategories: results[0].classifications[0].categories.slice(0, 3),
          categoriesOverTime,
          duration: audioBuffer.duration
        };
      } catch (err) {
        console.error(`Classification error for ${file.name}:`, err);
        throw new Error(`Failed to classify audio: ${err.message}`);
      }
    }

    async function analyzeSingleRecording(file) {
      console.log(`Analyzing single recording: ${file.name}`);
      if (!audioClassifier) {
        updateStatus("Audio Classifier still loading. Please try again.", true);
        console.error("Audio classifier not ready");
        return;
      }
      const tbody = document.querySelector("#singleFileTable tbody");
      tbody.innerHTML = "";
      updateStatus(`Analyzing single recording: ${baseName(file.name)}...`);
      try {
        const audioBuffer = await decodeFile(file);
        const sampleRate = audioCtx.sampleRate;
        const segmentLength = 0.975; // YAMNet's default window size in seconds
        const samplesPerSegment = Math.floor(segmentLength * sampleRate);
        const totalSamples = audioBuffer.length;
        const numSegments = Math.ceil(totalSamples / samplesPerSegment);
        singleRecordingSegments = [];
        let naturalSegments = 0;
        let manmadeSegments = 0;

        console.log(`Splitting into ${numSegments} segments`);
        for (let i = 0; i < numSegments; i++) {
          const startSample = i * samplesPerSegment;
          const endSample = Math.min(startSample + samplesPerSegment, totalSamples);
          const segmentSamples = endSample - startSample;
          if (segmentSamples < sampleRate * 0.1) { // Skip very short segments (<100ms)
            console.log(`Skipping segment ${i} (too short: ${segmentSamples} samples)`);
            continue;
          }
          const segmentBuffer = audioCtx.createBuffer(audioBuffer.numberOfChannels, segmentSamples, sampleRate);
          for (let channel = 0; channel < audioBuffer.numberOfChannels; channel++) {
            const segmentData = segmentBuffer.getChannelData(channel);
            const sourceData = audioBuffer.getChannelData(channel);
            for (let j = 0; j < segmentSamples; j++) {
              segmentData[j] = sourceData[startSample + j] || 0;
            }
          }
          try {
            const pcm = segmentBuffer.getChannelData(0);
        const results = await enhancedClassify(pcm, audioCtx.sampleRate);
            if (!results?.[0]?.classifications?.[0]?.categories) {
              console.warn(`No classification results for segment ${i}`);
              singleRecordingSegments.push({
                startTime: (startSample / sampleRate).toFixed(2),
                topLabel: "No prediction",
                score: 0,
                category: "‚Äî",
                audioBuffer: segmentBuffer
              });
              continue;
            }
            const topCategory = results[0].classifications[0].categories[0];
            const category = bucket(topCategory.categoryName);
            if (category === "Natural") naturalSegments++;
            else manmadeSegments++;
            singleRecordingSegments.push({
              startTime: (startSample / sampleRate).toFixed(2),
              topLabel: topCategory.categoryName,
              score: topCategory.score,
              category,
              audioBuffer: segmentBuffer
            });
            sendMIDI(topCategory.categoryName, topCategory.score);
            console.log(`Segment ${i}: ${singleRecordingSegments[singleRecordingSegments.length - 1].startTime}s, ${topCategory.categoryName} (${(topCategory.score*100).toFixed(0)}%), ${category}`);
          } catch (err) {
            console.error(`Classification error for segment ${i}:`, err);
            singleRecordingSegments.push({
              startTime: (startSample / sampleRate).toFixed(2),
              topLabel: "Error",
              score: 0,
              category: "‚Äî",
              audioBuffer: segmentBuffer
            });
          }
        }
        
       classifyBackground(singleRecordingSegments);
       drawBackgroundPie();


        // Update table
        tbody.innerHTML = "";
        singleRecordingSegments.forEach((segment, index) => {
          const row = document.createElement("tr");
          row.className = segment.category === "Natural" ? "natural" : segment.category === "Man-made" ? "manmade" : "";
          const safeLabel = segment.topLabel.replace(/[^a-zA-Z0-9]/g, '_').toLowerCase();
          const downloadFilename = `${baseName(file.name).replace(/\.[^/.]+$/, '')}_segment_${segment.startTime}s_${segment.category.toLowerCase().replace(' ', '-')}.wav`;
          row.innerHTML = `
            <td class="border border-gray-400 p-2">${segment.startTime}</td>
            <td class="border border-gray-400 p-2">${segment.topLabel} <small>(${(segment.score*100).toFixed(0)}%)</small></td>
            <td class="border border-gray-400 p-2">${segment.category}</td>
            <td class="border border-gray-400 p-2">
              <button class="download-segment-btn bg-purple-500 hover:bg-purple-600 text-white font-bold py-1 px-2 rounded text-sm" data-index="${index}">
                ‚¨á Download
              </button>
            </td>
          `;
          tbody.appendChild(row);
        });

        // Add event listeners for download buttons
        document.querySelectorAll('.download-segment-btn').forEach(button => {
          button.addEventListener('click', () => {
            const index = parseInt(button.getAttribute('data-index'));
            const segment = singleRecordingSegments[index];
            const safeLabel = segment.topLabel.replace(/[^a-zA-Z0-9]/g, '_').toLowerCase();
            const filename = `${baseName(file.name).replace(/\.[^/.]+$/, '')}_segment_${segment.startTime}s_${segment.category.toLowerCase().replace(' ', '-')}.wav`;
            downloadSegment(segment, filename);
               classifyBackground(singleRecordingSegments);
        drawBackgroundPie();
          });
        });

        // Update counts and pie chart
             naturalCount += naturalSegments;
        manmadeCount += manmadeCount;
        drawPie(naturalCount, manmadeCount);

        if (singleRecordingSegments.length === 0) {
          updateStatus("No valid segments analyzed. File may be too short or corrupted.", true);
          tbody.innerHTML = `<tr><td colspan="4" class="border border-gray-400 p-2 text-center">No segments analyzed</td></tr>`;
        } else {
          updateStatus(`Analysis complete: ${naturalSegments} Natural, ${manmadeSegments} Man-made segments in ${baseName(file.name)}.`);
        }
      } catch (err) {
        console.error(`Error analyzing ${file.name}:`, err);
        updateStatus(`Failed to analyze ${baseName(file.name)}: ${err.message}`, true);
        tbody.innerHTML = `<tr><td colspan="4" class="border border-gray-400 p-2 text-center">Error: ${err.message}</td></tr>`;
      }
    }

    async function startRealTimeAnalysis(durationMinutes) {
      console.log(`Starting real-time analysis for ${durationMinutes} minutes`);
      if (!audioClassifier) {
        updateStatus("Audio Classifier still loading. Please try again.", true);
        console.error("Audio classifier not ready");
        return;
      }
      if (realTimeRecording) {
        updateStatus("Real-time analysis already in progress. Stop it first.", true);
        console.warn("Real-time recording already active");
        return;
      }

      const durationSeconds = durationMinutes * 60;
      if (durationSeconds < 60 || durationSeconds > 86400) {
        updateStatus("Duration must be between 1 minute and 24 hours.", true);
        console.error(`Invalid duration: ${durationMinutes} minutes`);
        return;
      }

      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: { channelCount: 1 } });
        if (!audioCtx || audioCtx.state === "closed") {
          audioCtx = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
        } else if (audioCtx.state === "suspended") {
          await audioCtx.resume();
        }
        const sampleRate = audioCtx.sampleRate;
        const segmentLength = 0.975; // YAMNet's default window size in seconds
        const samplesPerSegment = Math.floor(segmentLength * sampleRate);
        const totalSamples = Math.floor(durationSeconds * sampleRate);
        const numSegments = Math.ceil(totalSamples / samplesPerSegment);
        realTimeSegments = [];
        let sampleOffset = 0;
        let channelData = new Float32Array(totalSamples);

        const source = audioCtx.createMediaStreamSource(stream);
        const processor = audioCtx.createScriptProcessor(16384, 1, 1);
        realTimeRecording = { stream, processor, source };

        processor.onaudioprocess = (e) => {
          const inputData = e.inputBuffer.getChannelData(0);
          for (let i = 0; i < inputData.length && sampleOffset < totalSamples; i++) {
            channelData[sampleOffset++] = inputData[i];
          }
          const remainingSeconds = Math.max(0, durationSeconds - (sampleOffset / sampleRate));
          document.getElementById("realTimeStatus").textContent = `Recording... ${remainingSeconds.toFixed(0)}s remaining`;
          if (sampleOffset >= totalSamples) {
            stopRealTimeAnalysis(true);
          }
        };

        source.connect(processor);
        processor.connect(audioCtx.destination);

        document.getElementById("startRealTimeBtn").classList.add("hidden");
        document.getElementById("stopRealTimeBtn").classList.remove("hidden");
        document.getElementById("realTimeStatus").textContent = `Recording... ${durationSeconds}s remaining`;
        updateStatus(`Started real-time analysis for ${durationMinutes} minute(s).`);

        realTimeTimeout = setTimeout(() => stopRealTimeAnalysis(true), durationSeconds * 1000);

        // Process segments after recording
        const processSegments = async () => {
          console.log(`Processing ${numSegments} segments from real-time recording`);
          updateStatus("Analyzing real-time recording...");
          const tbody = document.querySelector("#realTimeTable tbody");
          tbody.innerHTML = "";
          let naturalSegments = 0;
          let manmadeSegments = 0;

          // Create full recording buffer
          const fullBuffer = audioCtx.createBuffer(1, sampleOffset, sampleRate);
          fullBuffer.getChannelData(0).set(channelData.subarray(0, sampleOffset));
          document.getElementById("downloadFullRecordingBtn").disabled = false;
          document.getElementById("downloadFullRecordingBtn").classList.remove("hidden");

          for (let i = 0; i < numSegments; i++) {
            const startSample = i * samplesPerSegment;
            const endSample = Math.min(startSample + samplesPerSegment, sampleOffset);
            const segmentSamples = endSample - startSample;
            if (segmentSamples < sampleRate * 0.1) { // Skip very short segments (<100ms)
              console.log(`Skipping segment ${i} (too short: ${segmentSamples} samples)`);
              continue;
            }
            const segmentBuffer = audioCtx.createBuffer(1, segmentSamples, sampleRate);
            const segmentData = segmentBuffer.getChannelData(0);
            for (let j = 0; j < segmentSamples; j++) {
              segmentData[j] = channelData[startSample + j] || 0;
            }
            try {
              const pcm = segmentBuffer.getChannelData(0);
           const results = await enhancedClassify(pcm, audioCtx.sampleRate);
              if (!results?.[0]?.classifications?.[0]?.categories) {
                console.warn(`No classification results for segment ${i}`);
                realTimeSegments.push({
                  startTime: (startSample / sampleRate).toFixed(2),
                  topLabel: "No prediction",
                  score: 0,
                  category: "‚Äî",
                  audioBuffer: segmentBuffer
                });
                continue;
              }
              const topCategory = results[0].classifications[0].categories[0];
              const category = bucket(topCategory.categoryName);
              if (category === "Natural") naturalSegments++;
              else manmadeSegments++;
              realTimeSegments.push({
                startTime: (startSample / sampleRate).toFixed(2),
                topLabel: topCategory.categoryName,
                score: topCategory.score,
                category,
                audioBuffer: segmentBuffer
              });
              sendMIDI(topCategory.categoryName, topCategory.score);
              console.log(`Segment ${i}: ${realTimeSegments[realTimeSegments.length - 1].startTime}s, ${topCategory.categoryName} (${(topCategory.score*100).toFixed(0)}%), ${category}`);
            } catch (err) {
              console.error(`Classification error for segment ${i}:`, err);
              realTimeSegments.push({
                startTime: (startSample / sampleRate).toFixed(2),
                topLabel: "Error",
                score: 0,
                category: "‚Äî",
                audioBuffer: segmentBuffer
              });
            }
          }

          // Update table
 tbody.innerHTML = "";
realTimeSegments.forEach((segment, index) => {
  const row = document.createElement("tr");
  row.className =
    segment.category === "Natural"
      ? "natural"
      : segment.category === "Man-made"
      ? "manmade"
      : "";
  const downloadFilename = `realtime_segment_${segment.startTime}s_${segment.category
    .toLowerCase()
    .replace(" ", "-")}.wav`;

  row.innerHTML = `
    <td class="border border-gray-400 p-2">${segment.startTime}</td>
    <td class="border border-gray-400 p-2">${segment.topLabel} <small>(${(segment.score * 100).toFixed(0)}%)</small></td>
    <td class="border border-gray-400 p-2">${segment.category}</td>
    <td class="border border-gray-400 p-2 space-x-2">
      <button class="play-segment-btn bg-green-500 hover:bg-green-600 text-white font-bold py-1 px-2 rounded text-sm" data-index="${index}">
        üîä Play
      </button>
      <button class="download-segment-btn bg-purple-500 hover:bg-purple-600 text-white font-bold py-1 px-2 rounded text-sm" data-index="${index}">
        ‚¨á
      </button>
    </td>
  `;
  tbody.appendChild(row);
});

     document.querySelectorAll('#realTimeTable .play-segment-btn').forEach(btn => {
  btn.addEventListener('click', () => {
    const idx = parseInt(btn.dataset.index, 10);
    playSegment(realTimeSegments[idx]);
  });
});

document.querySelectorAll('#realTimeTable .download-segment-btn').forEach(btn => {
  btn.addEventListener('click', () => {
    const idx = parseInt(btn.dataset.index, 10);
    const segment = realTimeSegments[idx];
    const filename = `realtime_segment_${segment.startTime}s_${segment.category.toLowerCase().replace(' ', '-')}.wav`;
    downloadSegment(segment, filename);
  });
});

          // Add event listener for full recording download
          document.getElementById("downloadFullRecordingBtn").onclick = () => {
            try {
              const wavBuffer = bufferToWave(fullBuffer, fullBuffer.length);
              const blob = new Blob([wavBuffer], { type: 'audio/wav' });
              const url = URL.createObjectURL(blob);
              const a = document.createElement('a');
              a.href = url;
              a.download = `realtime_recording_${durationMinutes}min.wav`;
              document.body.appendChild(a);
              a.click();
              document.body.removeChild(a);
              URL.revokeObjectURL(url);
              updateStatus(`Downloaded full recording: realtime_recording_${durationMinutes}min.wav`);
            } catch (err) {
              console.error("Error downloading full recording:", err);
              updateStatus(`Failed to download full recording: ${err.message}`, true);
            }
          };

          // Update counts and pie chart
          naturalCount += naturalSegments;
          manmadeCount += manmadeSegments;
          drawPie(naturalCount, manmadeCount);

          if (realTimeSegments.length === 0) {
            updateStatus("No valid segments analyzed from real-time recording.", true);
            tbody.innerHTML = `<tr><td colspan="4" class="border border-gray-400 p-2 text-center">No segments analyzed</td></tr>`;
          } else {
            updateStatus(`Real-time analysis complete: ${naturalSegments} Natural, ${manmadeSegments} Man-made segments.`);
          }
        };

        realTimeRecording.processSegments = processSegments;
      } catch (err) {
        console.error("Microphone error:", err);
        updateStatus(`Failed to start real-time analysis: ${err.message}`, true);
        document.getElementById("realTimeStatus").textContent = "Error accessing microphone";
        stopRealTimeAnalysis(false);
      }
    }

    function stopRealTimeAnalysis(process = false) {
      if (realTimeRecording) {
        console.log("Stopping real-time analysis");
        realTimeRecording.processor.disconnect();
        realTimeRecording.source.disconnect();
        realTimeRecording.stream.getTracks().forEach(t => t.stop());
        if (realTimeTimeout) clearTimeout(realTimeTimeout);
        if (process && realTimeRecording.processSegments) {
          realTimeRecording.processSegments();
        }
        realTimeRecording = null;
        realTimeTimeout = null;
        document.getElementById("startRealTimeBtn").classList.remove("hidden");
        document.getElementById("stopRealTimeBtn").classList.add("hidden");
        document.getElementById("realTimeStatus").textContent = "Stopped. Enter duration and click Start.";
        if (!process) {
          updateStatus("Real-time analysis stopped.");
          document.querySelector("#realTimeTable tbody").innerHTML = "";
          document.getElementById("downloadFullRecordingBtn").disabled = true;
          document.getElementById("downloadFullRecordingBtn").classList.add("hidden");
        }
      }
    }

async function processLargeFile(file) {
    const chunkSize = 5 * 1024 * 1024; // 5 MB chunks
    const totalChunks = Math.ceil(file.size / chunkSize);
    let processedChunks = 0;
    let naturalSegments = 0;
    let manmadeSegments = 0;

    updateStatus(`Processing large file: ${baseName(file.name)}...`);

    for (let i = 0; i < totalChunks; i++) {
        const start = i * chunkSize;
        const end = Math.min(start + chunkSize, file.size);
        const chunk = file.slice(start, end);

        try {
            const arrayBuffer = await chunk.arrayBuffer();
            const audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
            const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
            const pcm = audioBuffer.getChannelData(0);
            const results = await enhancedClassify(pcm, audioContext.sampleRate);

            if (results?.[0]?.classifications?.[0]?.categories) {
                const topCategory = results[0].classifications[0].categories[0];
                const category = bucket(topCategory.categoryName);
                if (category === "Natural") naturalSegments++;
                else manmadeSegments++;
                
                // Update single file table
                const tbody = document.querySelector("#singleFileTable tbody");
                const row = document.createElement("tr");
                row.className = category === "Natural" ? "natural" : "manmade";
                const startTime = (start / chunkSize * 0.975).toFixed(2); // Approximate time based on chunk
                const downloadFilename = `${baseName(file.name)}_chunk_${i}.wav`;
                row.innerHTML = `
                    <td class="border border-gray-400 p-2">${startTime}</td>
                    <td class="border border-gray-400 p-2">${topCategory.categoryName} <small>(${(topCategory.score * 100).toFixed(0)}%)</small></td>
                    <td class="border border-gray-400 p-2">${category}</td>
                    <td class="border border-gray-400 p-2">
                        <button class="download-segment-btn bg-purple-500 hover:bg-purple-600 text-white font-bold py-1 px-2 rounded text-sm" data-index="${i}">
                            ‚¨á Download
                        </button>
                    </td>
                `;
                tbody.appendChild(row);

                // Add download button listener
                row.querySelector('.download-segment-btn').addEventListener('click', () => {
                    downloadSegment({ audioBuffer }, downloadFilename);
                });
            }

            // Update progress
            processedChunks++;
            const progress = Math.round((processedChunks / totalChunks) * 100);
            updateProgress(progress);

            await audioContext.close();
        } catch (error) {
            console.error(`Error processing chunk ${i + 1}:`, error);
            updateStatus(`Error processing chunk ${i + 1}: ${error.message}`, true);
        }
    }

    // Update pie chart
    naturalCount += naturalSegments;
    manmadeCount += manmadeSegments;
    drawPie(naturalCount, manmadeCount);

    // Update background pie chart
    classifyBackground(singleRecordingSegments);
    drawBackgroundPie();

    updateStatus(`Analysis complete: ${naturalSegments} Natural, ${manmadeSegments} Man-made segments in ${baseName(file.name)}.`);
}

document.getElementById('singleFileInput').addEventListener('change', async (e) => {
    const file = e.target.files[0];
    if (!file) {
        updateStatus('No file selected', true);
        return;
    }
    const sizeThreshold = 10 * 1024 * 1024; // 10MB
    if (file.size > sizeThreshold) {
        updateStatus('Processing large file...');
        await processLargeFile(file);
    } else {
        updateStatus('File selected, click Analyze to process.');
    }
});

async function analyzeAudioBuffer(audioBuffer) {
    // Implement your audio analysis logic here
    // For example, calculate the average amplitude
    const channelData = audioBuffer.getChannelData(0);
    const averageAmplitude = channelData.reduce((sum, value) => sum + Math.abs(value), 0) / channelData.length;
    return { averageAmplitude };
}

function updateAnalysisResults(result) {
    // Update your UI with the analysis results
    console.log('Analysis result:', result);
}

function updateProgress(progress) {
    const progressElement = document.getElementById('analysis-progress');
    progressElement.textContent = `Analysis Progress: ${progress}%`;
}


    async function handleFiles(files) {
      console.log("Handling files:", files.length);
      if (!audioClassifier) {
        updateStatus("Audio Classifier still loading. Please try again.", true);
        console.error("Audio classifier not ready");
        return;
      }
      analyzedFiles = []; // Clear previous files
      const audioFiles = [...files].filter(f => SUPPORTED_AUDIO_TYPES.includes(f.type));
      if (!audioFiles.length) {
        updateStatus("No valid audio files found. Please upload WAV or MP3 files.", true);
        console.warn("No valid audio files detected");
        return;
      }
      const tbody = document.querySelector("#fileTable tbody");
      tbody.innerHTML = "";
      updateStatus(`Processing ${audioFiles.length} file(s)...`);
      let failedFiles = [];
      let processedCount = 0;
      const processingPromises = audioFiles.map(async (file, index) => {
        console.log(`Preparing to process: ${file.name}`);
        const row = document.createElement("tr");
        row.innerHTML = `
          <td class="border border-gray-400 p-2">${baseName(file.name)}</td>
          <td class="border border-gray-400 p-2"></td>
          <td class="border border-gray-400 p-2">Processing...</td>
          <td class="border border-gray-400 p-2">...</td>
          <td class="border border-gray-400 p-2"></td>
        `;
        tbody.appendChild(row);
        try {
          const audioBuffer = await decodeFile(file);
          row.cells[1].appendChild(createWaveformCanvas(audioBuffer).container);
          const { topCategories, categoriesOverTime } = await processAudioFile(file, audioBuffer);
          const topCategory = topCategories[0];
          const category = bucket(topCategory.categoryName);
          row.cells[2].innerHTML = topCategories
            .map(c => `${c.categoryName} <small>(${(c.score*100).toFixed(0)}%)</small>`)
            .join("<br>");
          row.cells[3].textContent = category;
          row.className = category === "Natural" ? "natural" : "manmade";
          if (categoriesOverTime.length > 1) {
            row.cells[4].appendChild(createTimeChart(categoriesOverTime));
          } else {
            row.cells[4].textContent = "Single classification";
          }
          if (category === "Natural") naturalCount++;
          else manmadeCount++;
          drawPie(naturalCount, manmadeCount);
          processedCount++;
        } catch (err) {
          console.error(`Error processing ${file.name}:`, err);
          row.cells[2].textContent = `Error: ${err.message}`;
          row.cells[3].textContent = "‚Äî";
          row.cells[4].textContent = "‚Äî";
          failedFiles.push(`${baseName(file.name)} (${err.message})`);
        }
      });
      try {
        await Promise.all(processingPromises);
        if (processedCount === 0) {
          updateStatus(`No files could be processed. Failed: ${failedFiles.join(", ")}`, true);
        } else if (failedFiles.length > 0) {
          updateStatus(`Processed ${processedCount} file(s). Failed: ${failedFiles.join(", ")}`, true);
        } else {
          updateStatus(`Done processing ${processedCount} file(s).`);
        }
      } catch (err) {
        console.error("Unexpected error in processing files:", err);
        updateStatus("Unexpected error processing files. Check console for details.", true);
      }
 }

// Event listener to handle file input change
document.getElementById('folderInput').addEventListener('change', (e) => {
  if (e.target.files.length) {
    // Process the uploaded files
    handleFiles(e.target.files);

    // Scroll to the pie section after handling files
    const pieSection = document.querySelector('.pie-container');
    if (pieSection) {
      pieSection.scrollIntoView({ behavior: 'smooth' });
    }
  }
});
    const dropZone = document.getElementById("dropZone");
    const folderInput = document.getElementById("folderInput");
    ['dragenter', 'dragover'].forEach(eventName => {
      dropZone.addEventListener(eventName, (e) => {
        e.preventDefault();
        dropZone.classList.add('active');
      });
    });
    ['dragleave', 'drop'].forEach(eventName => {
      dropZone.addEventListener(eventName, (e) => {
        e.preventDefault();
        dropZone.classList.remove('active');
      });
    });
    dropZone.addEventListener('drop', (e) => {
      e.preventDefault();
      const files = Array.from(e.dataTransfer.files);
      console.log("Files dropped:", files.length);
      if (files.length) handleFiles(files);
    });
    folderInput.addEventListener('change', (e) => {
      console.log("Files selected via input:", e.target.files.length);
      if (e.target.files.length) handleFiles(e.target.files);
    });
    

// Training event listener with embedding extraction
/* ----------  Enhanced Training System  ---------- */
class AudioTrainer {
  constructor() {
    this.model = {
      version: "1.1",
      categories: new Map(), // name ‚Üí {embeddings:[], threshold: number, accuracy: number}
      negativeExamples: [] // Array of negative example embeddings
    };
    this.currentTraining = null;
  }

  async trainCategory(name, audioBuffers) {
    if (!name || !audioBuffers?.length) throw new Error("Invalid training data");
    if (!embedder) throw new Error("Feature extractor not loaded");

    const totalSteps = audioBuffers.length * 4; // original + 3 augmentations
    this.currentTraining = { name, totalSteps, completed: 0 };

    try {
      const embeddings = [];
      for (const buffer of audioBuffers) {
        const augmented = this.augmentSample(buffer);
        for (const sample of augmented) {
          const emb = await this.extractEmbedding(sample);
          embeddings.push(emb);
          this.updateProgress();
        }
      }
      const { threshold, meanDistance } = this.calculateThreshold(embeddings);
      const accuracy = this.validate(embeddings, threshold);

      this.model.categories.set(name, { embeddings, threshold, meanDistance, accuracy, sampleCount: audioBuffers.length });

      // Add negative examples to the model
      this.model.negativeExamples = this.model.negativeExamples.concat(embeddings);

      return { accuracy, meanDistance, sampleCount: audioBuffers.length };
    } finally {
      this.currentTraining = null;
    }
  }

  async addNegativeExamples(audioBuffers) {
    if (!audioBuffers?.length) throw new Error("No negative examples provided");
    if (!embedder) throw new Error("Feature extractor not loaded");

    const totalSteps = audioBuffers.length * 4; // original + 3 augmentations
    this.currentTraining = { name: "Negative", totalSteps, completed: 0 };

    try {
      const embeddings = [];
      for (const buffer of audioBuffers) {
        const augmented = this.augmentSample(buffer);
        for (const sample of augmented) {
          const emb = await this.extractEmbedding(sample);
          embeddings.push(emb);
          this.updateProgress();
        }
      }
      this.model.negativeExamples = this.model.negativeExamples.concat(embeddings);
    } finally {
      this.currentTraining = null;
    }
  }

  async extractEmbedding(audioBuffer) {
    const result = await embedder.embed(audioBuffer.getChannelData(0), audioBuffer.sampleRate);
    return result[0].embeddings[0].floatEmbedding;
  }

  augmentSample(buffer) {
    return [
      buffer,
      this.timeStretch(buffer, 0.9),
      this.timeStretch(buffer, 1.1),
      this.addNoise(buffer, 0.05)
    ];
  }

  calculateThreshold(embeddings) {
    const distances = [];
    for (let i = 0; i < embeddings.length; i++) {
      for (let j = i + 1; j < embeddings.length; j++) {
        distances.push(this.cosineDistance(embeddings[i], embeddings[j]));
      }
    }
    const mean = distances.reduce((a, b) => a + b, 0) / distances.length;
    const std = Math.sqrt(distances.map(d => Math.pow(d - mean, 2)).reduce((a, b) => a + b) / distances.length);
    return { threshold: mean + std * 1.5, meanDistance: mean };
  }

  validate(embeddings, threshold) {
    const holdout = embeddings.slice(-Math.floor(embeddings.length * 0.2));
    let correct = 0;
    for (const emb of holdout) {
      let minDist = Infinity;
      for (const other of embeddings) {
        if (other === emb) continue;
        const dist = this.cosineDistance(emb, other);
        if (dist < minDist) minDist = dist;
      }
      if (minDist <= threshold) correct++;
    }
    return correct / holdout.length;
  }

  cosineDistance(a, b) {
    let dot = 0, magA = 0, magB = 0;
    for (let i = 0; i < a.length; i++) { dot += a[i] * b[i]; magA += a[i] * a[i]; magB += b[i] * b[i]; }
    return 1 - (dot / (Math.sqrt(magA) * Math.sqrt(magB)));
  }

  timeStretch(buffer, factor) {
    const length = Math.floor(buffer.length / factor);
    const stretched = audioCtx.createBuffer(buffer.numberOfChannels, length, buffer.sampleRate);
    for (let c = 0; c < buffer.numberOfChannels; c++) {
      const src = buffer.getChannelData(c);
      const dst = stretched.getChannelData(c);
      for (let i = 0; i < length; i++) {
        const idx = Math.min(src.length - 1, i * factor);
        const frac = idx % 1;
        const idx0 = Math.floor(idx);
        const idx1 = Math.min(idx0 + 1, src.length - 1);
        dst[i] = src[idx0] * (1 - frac) + src[idx1] * frac;
      }
    }
    return stretched;
  }

  addNoise(buffer, level) {
    const noisy = audioCtx.createBuffer(buffer.numberOfChannels, buffer.length, buffer.sampleRate);
    for (let c = 0; c < buffer.numberOfChannels; c++) {
      const src = buffer.getChannelData(c);
      const dst = noisy.getChannelData(c);
      for (let i = 0; i < src.length; i++) {
        dst[i] = src[i] + (Math.random() * 2 - 1) * level;
      }
    }
    return noisy;
  }

  updateProgress() {
    if (!this.currentTraining) return;
    const percent = (++this.currentTraining.completed / this.currentTraining.totalSteps) * 100;
    updateTrainingProgress(percent);
  }
}

const audioTrainer = new AudioTrainer();

function updateTrainingProgress(percent) {
  const progress = document.querySelector('.training-progress');
  progress.style.display = percent > 0 ? 'flex' : 'none';
  progress.querySelector('progress').value = percent;
  progress.querySelector('.progress-text').textContent = `${Math.round(percent)}%`;
}

function updateCustomCategoriesList() {
  const listEl = document.getElementById('customCategoriesList');
  listEl.innerHTML = '';
  audioTrainer.model.categories.forEach((data, name) => {
    const li = document.createElement('li');
    li.innerHTML = `<strong>${name}</strong> <small>(acc: ${(data.accuracy * 100).toFixed(1)}%, samples: ${data.sampleCount})</small>`;
    listEl.appendChild(li);
  });
}

document.getElementById('startTrainingBtn').addEventListener('click', async () => {
  const name = document.getElementById('newCategoryName').value.trim();
  const files = Array.from(document.getElementById('trainingSamplesInput').files);

  if (!name) { alert("Please enter a category name"); return; }
  if (files.length < 3) { alert("Minimum 3 samples recommended (5+ for better accuracy)"); return; }

  try {
    const audioBuffers = [];
    for (const file of files) {
      try { audioBuffers.push(await decodeFile(file)); } catch (err) {
        console.error(`Skipping ${file.name}: ${err.message}`);
      }
    }
    if (audioBuffers.length < 3) throw new Error(`Only ${audioBuffers.length} valid files found`);

    const resultsEl = document.getElementById('trainingResults');
    resultsEl.innerHTML = `<div class="status-indicator">Training "${name}" with ${audioBuffers.length} samples...</div>`;

    const {accuracy, meanDistance} = await audioTrainer.trainCategory(name, audioBuffers);

    resultsEl.innerHTML = `
      <div class="status-indicator" style="border-color: ${accuracy > 0.7 ? '#4CAF50' : '#FFC107'}">
        Trained "${name}"<br>
        ‚Ä¢ Accuracy: ${(accuracy * 100).toFixed(1)}%<br>
        ‚Ä¢ Avg similarity: ${(1 - meanDistance).toFixed(3)}<br>
        ‚Ä¢ Samples: ${audioBuffers.length} (${audioBuffers.length * 4} augmented)
      </div>
    `;
    updateCustomCategoriesList();
    if (accuracy < 0.6) alert(`Warning: Low validation accuracy (${(accuracy * 100).toFixed(1)}%).\nTry adding more varied samples.`);
  } catch (err) {
    console.error("Training failed:", err);
    document.getElementById('trainingResults').innerHTML = `<div class="status-indicator" style="border-color:#F44336">Training failed: ${err.message}</div>`;
  } finally {
    updateTrainingProgress(0);
  }
});

document.getElementById('saveModelBtn').addEventListener('click', () => {
  if (audioTrainer.model.categories.size === 0) {
    alert('No trained model to save');
    return;
  }
  const modelData = JSON.stringify({
    version: audioTrainer.model.version,
    categories: Object.fromEntries(audioTrainer.model.categories),
    negativeExamples: audioTrainer.model.negativeExamples
  });
  const blob = new Blob([modelData], { type: 'application/json' });
  const url = URL.createObjectURL(blob);
  const a = document.createElement('a');
  a.href = url;
  a.download = 'sound_classifier_model.json';
  a.click();
  URL.revokeObjectURL(url);
});

document.getElementById('loadModelBtn').addEventListener('click', () => {
  const input = document.createElement('input');
  input.type = 'file';
  input.accept = '.json';
  input.onchange = e => {
    const file = e.target.files[0];
    if (!file) return;
    const reader = new FileReader();
    reader.onload = event => {
      try {
        const data = JSON.parse(event.target.result);
        audioTrainer.model.categories = new Map(Object.entries(data.categories));
        audioTrainer.model.negativeExamples = data.negativeExamples || [];
        updateCustomCategoriesList();
        alert(`Loaded model with ${audioTrainer.model.categories.size} categories`);
      } catch (err) {
        alert('Invalid model file');
      }
    };
    reader.readAsText(file);
  };
  input.click();
});

    const singleFileInput = document.getElementById("singleFileInput");
    const analyzeSingleBtn = document.getElementById("analyzeSingleBtn");
    
   document.getElementById('singleFileInput').addEventListener('change', async (e) => {
    const file = e.target.files[0];
    if (!file) {
        updateStatus('No file selected', true);
        return;
    }
    updateStatus('Processing large file...');
    await processLargeFile(file);
});

    const startRealTimeBtn = document.getElementById("startRealTimeBtn");
    const stopRealTimeBtn = document.getElementById("stopRealTimeBtn");
    const realTimeDuration = document.getElementById("realTimeDuration");

    startRealTimeBtn.addEventListener('click', () => {
      const duration = parseFloat(realTimeDuration.value);
      if (isNaN(duration) || duration < 1 || duration > 1440) {
        updateStatus("Please enter a duration between 1 and 1440 minutes.", true);
        return;
      }
      startRealTimeAnalysis(duration);
    });

    stopRealTimeBtn.addEventListener('click', () => {
      stopRealTimeAnalysis(true);
    });

    const micBtn = document.getElementById("micBtn");
    const micOut = document.getElementById("micResult");
    let micStream, micProc;

    micBtn.onclick = async () => {
      if (!audioClassifier) {
        micOut.textContent = "Audio Classifier still loading. Please try again";
        console.error("Audio classifier not ready for microphone");
        return;
      }
      if (micProc) {
        micProc.disconnect();
        if (audioCtx) await audioCtx.close();
        micStream?.getTracks().forEach(t => t.stop());
        micProc = null;
        micBtn.textContent = "üé§ Record Live Audio";
        micOut.textContent = "(stopped)";
        return;
      }
      try {
        micStream = await navigator.mediaDevices.getUserMedia({audio: true});
        if (!audioCtx || audioCtx.state === "closed") {
          audioCtx = new (window.AudioContext || window.webkitAudioContext)({sampleRate: 16000});
        } else if (audioCtx.state === "suspended") {
          await audioCtx.resume();
        }
        const source = audioCtx.createMediaStreamSource(micStream);
        micProc = audioCtx.createScriptProcessor(16384, 1, 1);
        micProc.onaudioprocess = async e => {
      const inputData = e.inputBuffer.getChannelData(0);
          try {
     const results = await enhancedClassify(inputData, audioCtx.sampleRate);
            if (!results?.[0]?.classifications?.[0]?.categories) {
              micOut.textContent = "No confident prediction";
              return;
            }
            const categories = results[0].classifications[0].categories;
            const top = categories[0];
            const cat = bucket(top.categoryName);
            if (cat === "Natural" && top.score >= 0.8) {
              naturalChallengeProgress++;
              document.getElementById("naturalChallengeProgress").textContent = `${naturalChallengeProgress}/5`;
              localStorage.setItem("naturalChallengeProgress", naturalChallengeProgress);
            }
            micOut.innerHTML = `
              <strong>${top.categoryName}</strong> 
              <small>(${(top.score*100).toFixed(0)}%)</small> ‚Üí 
              <span style="color:${cat === 'Natural' ? '#4CAF50' : '#F44336'}">${cat}</span>
              <br>
              <small>Also: ${categories.slice(1, 3).map(c => `${c.categoryName} (${(c.score*100).toFixed(0)}%)`).join(', ')}</small>
            `;
            if (cat === "Natural") naturalCount++;
            else manmadeCount++;
            drawPie(naturalCount, manmadeCount);
          } catch (err) {
            console.error("Classification error:", err);
            micOut.textContent = "Error processing audio";
          }
        };
        source.connect(micProc);
        micProc.connect(audioCtx.destination);
        micBtn.textContent = "‚èπ Stop";
        micOut.textContent = "Listening...";
      } catch (err) {
        console.error("Microphone error:", err);
        micOut.textContent = "Error accessing microphone";
      }
    };

    // Slider Event Listeners
    document.getElementById("fadeSlider").addEventListener('input', (e) => {
      fadeDuration = parseFloat(e.target.value);
      document.getElementById("fadeValue").textContent = fadeDuration.toFixed(1);
    });

    document.getElementById("speedSlider").addEventListener('input', (e) => {
      playbackSpeed = parseFloat(e.target.value);
      document.getElementById("speedValue").textContent = playbackSpeed.toFixed(1);
      if (combinedWaveformInstance) {
        combinedWaveformInstance.setPlaybackRate(playbackSpeed);
      }
    });

    // Render and Download Buttons
    document.getElementById("renderBtn").addEventListener('click', combineAudioFiles);
    document.getElementById("downloadBtn").addEventListener('click', downloadCombinedAudio);

    // Play/Pause Button
document.getElementById("playBtn").addEventListener('click', async () => {
  // temporary debug
  console.log('audioCtx.state before resume:', audioCtx?.state);

  if (audioCtx?.state === 'suspended') await audioCtx.resume();
  combinedWaveformInstance.playPause();
});

document.getElementById('openHelpBtn').addEventListener('click', () =>
  document.getElementById('helpSection').style.display = 'block'
);

document.getElementById('closeHelpBtn').addEventListener('click', () =>
  document.getElementById('helpSection').style.display = 'none'
);

   // event listener to update the display value in seconds
        document.getElementById('lenSlider').addEventListener('input', (e) => {
            const value = parseInt(e.target.value);
            document.getElementById('lenValue').textContent = value;
        });


    // Check if running on file:// protocol
    if (window.location.protocol === 'file:') {
      updateStatus("Error: This app must be run on a web server (http:// or https://). Use 'python -m http.server 8000' or deploy to a hosting service. Some features may not work.", true);
      console.error("Running on file:// protocol. Service worker and manifest will not load.");
    }

    // Initialize everything
    console.log("Initializing application");
    initializeAudioClassifier();
    drawPie(0, 0);



  </script>
    <script>
/* ----------  Play / Pause / Stop controls ---------- */
const playBtn   = document.getElementById('playBtn');
const stopBtn   = document.createElement('button');



let playbackSrc = null;   // keep the last source so we can stop it

playBtn.addEventListener('click', async () => {
  if (!combinedBuffer) return;
  if (audioCtx?.state === 'suspended') await audioCtx.resume();

  const stretch = parseFloat(document.getElementById('stretchSlider').value) || 1;
  const duration = combinedBuffer.duration / stretch;

  // create new source every time
  if (playbackSrc) playbackSrc.stop();
  playbackSrc = audioCtx.createBufferSource();
  playbackSrc.buffer = combinedBuffer;
  playbackSrc.playbackRate.value = stretch;
  playbackSrc.connect(audioCtx.destination);
  playbackSrc.start();

  playBtn.textContent = '‚è∏ Pause';
  playbackSrc.onended = () => { playBtn.textContent = '‚ñ∂ Play'; playbackSrc = null; };
});

stopBtn.addEventListener('click', () => {
  if (playbackSrc) { playbackSrc.stop(); playbackSrc = null; }
  playBtn.textContent = '‚ñ∂ Play';
});
      
window.combineAudioFiles = async function () {
  if (!analyzedFiles.length) { updateStatus("No audio files to combine.", true); return; }
  updateStatus("Building render‚Ä¶");

  const valid = analyzedFiles.filter(f => f.audioBuffer);
  if (!valid.length) { updateStatus("No valid audio buffers.", true); return; }

  /* 1. force mono */
  const mono = [];
  for (const v of valid) {
    const src = v.audioBuffer;
    const buf = audioCtx.createBuffer(1, src.length, src.sampleRate);
    const out = buf.getChannelData(0);
    if (src.numberOfChannels === 1) {
      out.set(src.getChannelData(0));
    } else {
      const ch0 = src.getChannelData(0);
      const ch1 = src.getChannelData(1) || new Float32Array(src.length);
      for (let i = 0; i < src.length; i++) out[i] = (ch0[i] + ch1[i]) * 0.5;
    }
    v.audioBuffer = buf;
    mono.push(v);
  }
  valid.splice(0, valid.length, ...mono);

  /* 2. order */
  const mode = document.getElementById('renderMode').value;
  let ordered = [...valid];
  const rng = () => Math.random();
  const lorenz = (x,y,z,s=10,r=28,b=2.667,dt=0.01)=>{
    const dx=s*(y-x), dy=x*(r-z)-y, dz=x*y-b*z;
    return [x+dx*dt,y+dy*dt,z+dz*dt];
  };
  const buildMarkov = (segments)=>{
    const st={natural:[],manmade:[]};
    segments.forEach(s=>st[s.category].push(s));
    return k=>st[k][Math.floor(rng()*st[k].length)];
  };
  switch(mode){
    case 'low‚Üíhigh': ordered.sort((a,b)=>a.frequency-b.frequency);break;
    case 'high‚Üílow': ordered.sort((a,b)=>b.frequency-a.frequency);break;
    case 'alt':{
      const n=ordered.filter(s=>s.category==='Natural');
      const m=ordered.filter(s=>s.category==='Man-made');
      ordered=[]; const l=Math.max(n.length,m.length);
      for(let i=0;i<l;i++){ if(n[i])ordered.push(n[i]); if(m[i])ordered.push(m[i]); }
      break;
    }
    case 'random':
      for(let i=ordered.length-1;i>0;i--){ const j=Math.floor(rng()*(i+1)); [ordered[i],ordered[j]]=[ordered[j],ordered[i]]; }
      break;
    case 'lorenz':{
      let [x,y,z]=[0.1,0,0]; const idxs=[];
      for(let i=0;i<ordered.length;i++){ [x,y,z]=lorenz(x,y,z); idxs.push(Math.floor(Math.abs(x)*1000)%ordered.length); }
      ordered=idxs.map(i=>ordered[i]); break;
    }
    case 'markov':{
      const next=buildMarkov(ordered); let cur=ordered[0].category;
      const seq=[ordered[0]];
      for(let i=1;i<ordered.length;i++){ const pick=next(cur); if(pick) seq.push(pick); cur=pick.category; }
      ordered=seq; break;
    }
  }

  /* 3. duration target */
  const targetSec = parseInt(document.getElementById('lenSlider').value, 10);
  const singleDur = ordered.reduce((s,f)=>s+f.audioBuffer.duration, 0);
  const loops = Math.ceil(targetSec / singleDur);
  const totalDur = loops * singleDur;
  const ch = 1;
  const sr = ordered[0].audioBuffer.sampleRate;
  const buf = audioCtx.createBuffer(ch, Math.floor(totalDur * sr), sr);

  /* 4. time-stretch */
  const stretch = parseFloat(document.getElementById('stretchSlider').value) || 1;
  const fadeSamples = parseFloat(document.getElementById('fadeSlider').value) * sr;

  /* 5. fill buffer (with looping) */
  let offset = 0;
  for (let l = 0; l < loops; l++) {
    for (const file of ordered) {
      const src = file.audioBuffer;
      const srcDur = src.length;
      const needed = Math.floor(srcDur / stretch);
      const out = buf.getChannelData(0);
      for (let i = 0; i < needed; i++) {
        const idx = Math.min(srcDur - 1, Math.floor(i * stretch));
        let gain = 1;
        if (i < fadeSamples) gain = i / fadeSamples;
        if (i > needed - fadeSamples) gain = (needed - i) / fadeSamples;
        out[offset + i] = (out[offset + i] || 0) + src.getChannelData(0)[idx] * gain;
      }
      offset += needed;
    }
  }

  /* 6. waveform / buttons */
  console.log("Rendering waveform");
  document.getElementById("combinedWaveform").innerHTML = "";
  const { container, wavesurfer } = createWaveformCanvas(buf);
  document.getElementById("combinedWaveform").appendChild(container);
  combinedWaveformInstance = wavesurfer;
  combinedBuffer = buf;
  document.getElementById("playBtn").disabled = false;
  document.getElementById("downloadBtn").disabled = false;
  updateStatus("Combined audio rendered successfully!");
};

      //////////////////////////////////////////////////////////////////
//  MIDI MAPPER
//////////////////////////////////////////////////////////////////
let midiOut = null;
const midiRows = [];

// 1.  Port selector
navigator.requestMIDIAccess && navigator.requestMIDIAccess().then(acc => {
  const sel = document.getElementById('midiPortSelect');
  [...acc.outputs.values()].forEach(p => {
    const opt = document.createElement('option');
    opt.value = p.id;
    opt.textContent = p.name;
    sel.appendChild(opt);
  });
  midiOut = [...acc.outputs.values()][0];
  sel.addEventListener('change', e => midiOut = acc.outputs.get(e.target.value));
});

// 2.  Row factory
function addMidiRow(label = '', note = 60, cc = 1) {
  const id = Date.now();
  const div = document.createElement('div');
  div.className = 'midi-row';
  div.style.cssText = 'display:flex;gap:8px;align-items:center';
  div.innerHTML = `
    <input placeholder="Label" value="${label}" style="flex:1">
    <label>Note<input type="number" min="0" max="127" value="${note}" style="width:50px"></label>
    <label>CC<input type="number" min="0" max="119" value="${cc}" style="width:50px"></label>
    <button onclick="this.parentElement.remove()">‚úï</button>
  `;
  document.getElementById('midiRows').appendChild(div);
  midiRows.push({ el: div });
}
      
 document.getElementById('checkMidiBtn').addEventListener('click', checkMIDIOut);

function checkMIDIOut() {
  if (!navigator.requestMIDIAccess) {
    alert('MIDI is not supported in your browser.');
    return;
  }

  navigator.requestMIDIAccess().then(onMIDISuccess, onMIDIFailure);
}

function onMIDISuccess(midiAccess) {
  const outputs = midiAccess.outputs.values();
  let midiOut = null;

  for (let output of outputs) {
    midiOut = output;
    break; // Assuming we only need the first output port
  }

  if (!midiOut) {
    alert('No MIDI output ports found.');
    return;
  }

  // Send a test MIDI message
  const noteOn = [0x90, 60, 127]; // Note On, middle C, full velocity
  const noteOff = [0x80, 60, 0]; // Note Off, middle C

  midiOut.send(noteOn, performance.now());
  midiOut.send(noteOff, performance.now() + 500); // Note off after 500ms

  alert('Test MIDI message sent. Check your MIDI device.');
}

function onMIDIFailure() {
  alert('Failed to get MIDI access.');
}



// 3.  Send routine ‚Äì called by classification
function sendMIDI(label, conf) {
  if (!midiOut) return;
  const row = midiRows.find(r => r.el.children[0].value === label);
  if (!row) return;
  const note = +row.el.children[1].children[0].value;
  const cc   = +row.el.children[2].children[0].value;
  const vel  = Math.round(conf * 127);
  midiOut.send([0x90, note, vel]);
  midiOut.send([0x80, note, 0], performance.now() + 200);
  midiOut.send([0xB0, cc, vel]);
}

// 4.  UI hooks
document.getElementById('addMidiRowBtn').addEventListener('click', addMidiRow);

// 5.  Classification sources that drive MIDI
// Folder, Single Recording, Real-Time & Live Mic all call:
// sendMIDI(label, confidence)   ‚Üê insert this line inside each classify callback
        
            // Service Worker Registration
    if ('serviceWorker' in navigator && window.location.protocol !== 'file:') {
      navigator.serviceWorker.register('/sw.js').then(reg => {
        console.log('Service Worker registered:', reg);
      }).catch(err => {
        console.error('Service Worker registration failed:', err);
        updateStatus("Failed to register service worker. Some offline features may be unavailable.", true);
      });
    }
          </script>
  </body>
</html>
